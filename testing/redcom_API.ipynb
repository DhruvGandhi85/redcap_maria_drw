{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations          # allows for more strict type hinting (documentation)\n",
    "import dotenv           # loads system environment\n",
    "import os               # accesses system environment for stored private variables\n",
    "import gc               # garbage collection for memory management\n",
    "import mariadb          # create connection to mdb server\n",
    "import sys              # kills process if necessary\n",
    "import re               # regex for data manipulation \n",
    "import socket           # retrieves IP for log entry\n",
    "import datetime         # for timestamp field \n",
    "import time             # allows sleep intervals between polling\n",
    "import pandas as pd     # data manipulation\n",
    "import numpy as np      # converting numeric types for comparison\n",
    "import warnings         # suppresses deprecation warnings\n",
    "import logging          # logs events\n",
    "import smtplib          # sends email alerts\n",
    "\n",
    "from logging.config import dictConfig               # allows for logging configuration\n",
    "from email.mime.text import MIMEText                # formats email alerts\n",
    "from email.mime.multipart import MIMEMultipart      # formats email alerts\n",
    "\n",
    "from scipy.special import erfc                      # provides mathematical operations for outlier filtering (Chauvenetâ€™s)\n",
    "from scipy.stats import norm                        # provides mathematical operations for outlier filtering (QQ)\n",
    "from sklearn.preprocessing import StandardScaler    # provides mathematical operations for outlier filtering (QQ)\n",
    "from statsmodels.formula.api import ols             # provides mathematical operations for outlier filtering (QQ)\n",
    "import statsmodels.api as sm                        # provides mathematical operations for outlier filtering (QQ)\n",
    "\n",
    "dotenv.load_dotenv()                                # loads system environment variables from .env file\n",
    "warnings.filterwarnings('ignore')                   # suppresses deprecation warnings \n",
    "\n",
    "pd.set_option('display.max_columns', None)          # sets pandas output view settings (for development)\n",
    "pd.set_option('display.expand_frame_repr', False)   # sets pandas output view settings (for development)\n",
    "pd.set_option('max_colwidth', -1)                   # sets pandas output view settings (for development)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = f\"C:\\\\inetpub\\\\flaskApp\"            # sets root directory\n",
    "logdir = fr\"{rootdir}\\\\output_logs\\\\{datetime.datetime.now().strftime('%Y-%m')}\"                # sets log directory\n",
    "logfile = fr\"{logdir}\\\\redcom_log_{datetime.datetime.now().strftime('%Y-%m-%d')}.log\"           # sets log file\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "dictConfig({\n",
    "    'version': 1,\n",
    "    'formatters': {\n",
    "        'default': {\n",
    "            'format': '[%(asctime)s]: %(message)s',\n",
    "            'datefmt': '%Y-%m-%d %H:%M:%S',\n",
    "        },\n",
    "    },\n",
    "    'handlers': {\n",
    "        'wsgi': {\n",
    "            'class': 'logging.StreamHandler',\n",
    "            'formatter': 'default',\n",
    "        },\n",
    "        'file': {\n",
    "            'class': 'logging.FileHandler',\n",
    "            'formatter': 'default',\n",
    "            'filename': logfile,\n",
    "            'encoding': 'utf-8',\n",
    "        },\n",
    "    },\n",
    "    'root': {\n",
    "        'level': 'INFO',\n",
    "        'handlers': ['wsgi', 'file'],\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log_file(logfile: str) -> str:\n",
    "    \"\"\"\n",
    "    For use in the Flask app, reads the last 10 lines of the log file and returns them as HTML.\n",
    "\n",
    "    Args:\n",
    "        logfile (str): The path to the log file.\n",
    "\n",
    "    Returns:\n",
    "        str: The last 10 lines of the log file as HTML.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(logfile, 'r') as file:\n",
    "            lines = file.readlines()[-10:]\n",
    "        styled_lines = [\n",
    "            f\"<span style='background-color: { '#f0f0f0' if i % 2 == 0 else '#ffffff' }; display: block;'>{line.strip()}</span>\"\n",
    "            for i, line in enumerate(lines)\n",
    "        ]\n",
    "        return \"\".join(styled_lines)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading log file: {e}\")\n",
    "        return \"Could not read log file.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email(message: str = '', recipient_emails: list = [os.environ.get(\"adminEmail\")]) -> str:\n",
    "    \"\"\"\n",
    "    Sends an email alert to the specified recipient(s) with the specified message.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to include in the email.\n",
    "        recipient_emails (list): A list of email addresses to send the email to.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating that the email was sent.\n",
    "    \"\"\"\n",
    "    sender_email = \"donotreply@redcap.hnrc.tufts.edu\"\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = \", \".join(recipient_emails) if isinstance(recipient_emails, list) else recipient_emails\n",
    "    msg['Subject'] = \"Redcap Alert\"\n",
    "    body = (\n",
    "        \"This is an automated message. Please do not reply to this email.\\n\\n\"\n",
    "        f\"{message}\\n\\n\"\n",
    "    )\n",
    "    msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "    # Tufts email server and port\n",
    "    with smtplib.SMTP(\"130.64.49.7\", 25) as server:\n",
    "        server.ehlo()\n",
    "        server.sendmail(sender_email, recipient_emails, msg.as_string())\n",
    "\n",
    "    return 'Email sent \\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_error_email(message: str = '', recipient_emails: list = [os.environ.get(\"adminEmail\")]) -> str:\n",
    "    \"\"\"\n",
    "    Sends an error email alert to the specified recipient(s) with the specified message.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to include in the email.\n",
    "        recipient_emails (list): A list of email addresses to send the email to.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating that the email was sent.\n",
    "    \"\"\"\n",
    "    sender_email = \"donotreply@redcap.hnrc.tufts.edu\"\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = \", \".join(recipient_emails) if isinstance(recipient_emails, list) else recipient_emails\n",
    "    msg['Subject'] = \"Automation Process Alert\"\n",
    "    body = (\n",
    "        \"We encountered an issue while attempting to execute the automation process. \"\n",
    "        \"Please review the system logs or contact the support team to address the issue.\\n\\n\"\n",
    "        \"This is an automated message. Please do not reply to this email.\\n\\n\"\n",
    "        f\"{message}\\n\\n\"\n",
    "    )\n",
    "    msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "    # Tufts email server and port\n",
    "    with smtplib.SMTP(\"130.64.49.7\", 25) as server:\n",
    "        server.ehlo()\n",
    "        server.sendmail(sender_email, recipient_emails, msg.as_string())\n",
    "\n",
    "    return 'Email sent \\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_last_run(hours: int = 9) -> None:\n",
    "    \"\"\"\n",
    "    Checks the last time the outlier and missing data routine was run. If it was more than n hours ago, sends an email to the administrator.\n",
    "\n",
    "    Args:\n",
    "        hours (int, optional): The number of hours to check against.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open('stored_data/last_routine.log', 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    \n",
    "    timestamp = datetime.datetime.strptime(file_contents, '%Y-%m-%d %H:%M:%S')\n",
    "    time_diff = datetime.datetime.now() - timestamp\n",
    "\n",
    "    if time_diff > datetime.timedelta(hours=hours):\n",
    "        send_error_email(message=f'Last run was more than {hours} hours ago. Please check the server.')\n",
    "    logging.info(\"Time since last run: \" + str(time_diff))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_maria(maria_user: str = os.environ.get(\"mariaUser\"), maria_pass: str = os.environ.get(\"mariaPass\"), maria_host: str = os.environ.get(\"mariaHost\"), maria_database: str = os.environ.get(\"mariaDatabase\")) -> mariadb.connections.Connection:\n",
    "    \"\"\"\n",
    "    Establishes a connection to the mariaDB server.\n",
    "\n",
    "    If the arguments `maria_user`, `maria_pass`, `maria_host`, `maria_database` are not provided,\n",
    "    the default credentials from the `.env` file are loaded in. \n",
    "\n",
    "    Args:\n",
    "        maria_user (str, optional): The username of the mariaDB account (default is `mariaUser` from `.env`)\n",
    "        maria_pass (str, optional): The password of the mariaDB account (default is `mariaPass` from `.env`)\n",
    "        maria_host (str, optional): The host of the mariaDB server (default is `mariaHost` from `.env`)\n",
    "        maria_database (str, optional): The name of the mariaDB database (default is `mariaDatabase` from `.env`)\n",
    "\n",
    "    Returns:\n",
    "        mariadb.connections.Connection: A connection to the mariaDB server.\n",
    "\n",
    "    Raises:    \n",
    "        mariadb.Error: Raised if the connection fails (e.g. incorrect credentials or server is at max capacity)\n",
    "    \"\"\"\n",
    "\n",
    "    # sourced from mariaDB website - creates a connection to the mariaDB linux server \n",
    "    try:                              \n",
    "        conn = mariadb.connect(\n",
    "            user=maria_user,\n",
    "            password=maria_pass,\n",
    "            host=maria_host,\n",
    "            database=maria_database\n",
    "        )\n",
    "        \n",
    "        # Disable auto-commit, changes must be committed manually\n",
    "        conn.autocommit = False       \n",
    "    except mariadb.Error as e:\n",
    "        logging.info(f\"Error connecting to MariaDB Platform: {e}\")\n",
    "\n",
    "        # terminate the process if there is a connection error\n",
    "        sys.exit(1)                 \n",
    "    \n",
    "    return conn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_maria_cmd(conn: mariadb.connections.Connection, sql_comm: str, data_input: tuple = None) -> list | None:\n",
    "    \"\"\"\n",
    "    Utilizes a cursor to execute a given SQL command in the mariaDB database. \n",
    "    If additional data is passed in, this is executed alongside the command as necessary\n",
    "\n",
    "    If the argument `data_input` is not provided, it is assumed this is not a data input operation.\n",
    "\n",
    "    Args:\n",
    "        conn (mariadb.connections.Connection): The active connection to the mariaDB server.\n",
    "        sql_comm (str): The SQL command to be executed in the database.\n",
    "        data_input (tuple, optional): The potential data values to be inputted into a mariaDB table, formatted as follows: `(val_col1, val_col2, val_col3, val_col...)`. Default value is None.\n",
    "\n",
    "    Returns:\n",
    "        list | None: If query returns a table, returns table values in the form of a list. Otherwise does not return anything.\n",
    "\n",
    "    Raises:\n",
    "        mariadb.Error: Raised if the operation fails (e.g., key-error, incorrect data values/types, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    # initializes a cursor to navigate mariaDB\n",
    "    cur = conn.cursor()\n",
    "    try: \n",
    "        # if data_input exists, submit the data alongside the execution of the command  \n",
    "        if data_input is not None:\n",
    "            cur.execute(sql_comm, data_input)\n",
    "        else:\n",
    "            cur.execute(sql_comm)\n",
    "        \n",
    "        # if query returns a table, returns table values in the form of a list\n",
    "        try:\n",
    "            results = cur.fetchall()\n",
    "            return results\n",
    "        except: \n",
    "            return None\n",
    "    \n",
    "    except mariadb.Error as e: \n",
    "        if \"Duplicate entry\" in str(e):\n",
    "            with open(fr'{rootdir}\\\\output_logs\\\\key_errors.csv', 'a') as f:\n",
    "                f.write(f\"{e}|{data_input}|{sql_comm}\\n\")  \n",
    "        else:\n",
    "           logging.info(f\"Error: {e} with data: {data_input} in command: {sql_comm}\")  \n",
    "           send_error_email(message=f\"Error: {e} with data: {data_input} in command: {sql_comm}\")  \n",
    "\n",
    "    # cursor MUST be closed at end of process so it is not infinitely hanging\n",
    "    finally:\n",
    "        cur.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colnames(conn: mariadb.connections.Connection, table_names: list) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves column names of given tables to use in data manipulation\n",
    "\n",
    "    Args:\n",
    "        conn (mariadb.connections.Connection): The active connection to the mariaDB server.\n",
    "        table_names (list): The names of the redcap tables to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of type `string: list_of_strings` representing redcap tables and their list of column names, formatted as follows: `{'table_1': ['t1_col1', 't1_c2'], 'table_2': ['t2_col1'], ...}`.\n",
    "    \"\"\"\n",
    "    table_cols = {}\n",
    "    for table_name in table_names:\n",
    "        sql_comm = f\"SHOW columns FROM {table_name}\"\n",
    "        results = execute_maria_cmd(conn, sql_comm)\n",
    "        table_attributes = pd.DataFrame(results)\n",
    "\n",
    "        # 0th element of returned table_attributes is colnames, so convert entire 0th column to a list\n",
    "        colnames = list(table_attributes[0])\n",
    "        table_cols[table_name] = colnames\n",
    "    return table_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_data(conn: mariadb.connections.Connection, table_cols: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves column names of given tables to use in data manipulation\n",
    "\n",
    "    Args:\n",
    "        conn (mariadb.connections.Connection): The active connection to the mariaDB server.\n",
    "        table_cols (dict): A dictionary of type `string: list_of_strings` representing redcap tables and their list of column names\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of type `string: pandas.DataFrame` representing redcap tables and their data, formatted as follows: `{'table_1': pd.DataFrame, 'table_2': pd.DataFrame, ...}`.\n",
    "    \"\"\"\n",
    "    tables = {}\n",
    "    table_names = list(table_cols.keys())\n",
    "    col_names = list(table_cols.values())\n",
    "    for table in range(len(table_names)):\n",
    "        sql_comm = f\"SELECT * FROM {table_names[table]}\"\n",
    "        results = execute_maria_cmd(conn, sql_comm)\n",
    "\n",
    "        df = pd.DataFrame(results, columns = col_names[table])\n",
    "        tables[table_names[table]] = df\n",
    "    return tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_database_table(table_names: list) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves column names of given tables to use in data manipulation\n",
    "\n",
    "    Args:\n",
    "        table_names (list): The names of the redcap tables to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of type `string: pd.DataFrame` representing redcap tables and their data, formatted as follows: `{'table_1': pd.DataFrame(table_1), 'table_2': pd.DataFrame(table_2), ...}`.\n",
    "    \"\"\"\n",
    "    conn = connect_to_maria()\n",
    "    table_cols = get_colnames(conn, table_names)\n",
    "    tables = get_table_data(conn, table_cols)\n",
    "    conn.close()\n",
    "    return tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dictionary(filter: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the data dictionary from the redcap_metadata table in the mariaDB server.\n",
    "    Filters the data dictionary to compare all ints and floats aside from ID number.\n",
    "\n",
    "    Args:\n",
    "        filter (bool, optional): If True, filters the data dictionary to compare all ints and floats aside from ID number. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the filtered data dictionary.\n",
    "    \"\"\"\n",
    "    table_data = retrieve_database_table(['redcap_metadata'])\n",
    "    data_dic = table_data['redcap_metadata']\n",
    "\n",
    "    data_dic = data_dic[~(data_dic['field_name'] == 'hnrcid')]\n",
    "    if filter:\n",
    "        data_dic = data_dic[(data_dic['element_validation_type'].str.contains('int', case=False, na=False)) | \n",
    "                            (data_dic['element_validation_type'].str.contains('float', case=False, na=False))]\n",
    "    \n",
    "    return data_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data_dictionary() -> None:\n",
    "    \"\"\"\n",
    "    Stores the data dictionary locally as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    path = f'{rootdir}\\\\stored_data'\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    data_dictionary = get_data_dictionary(filter=False)\n",
    "    data_dictionary['branching_logic'] = data_dictionary['branching_logic'].str.replace('\\n', ' ')\n",
    "\n",
    "    data_dictionary.to_csv(f'{path}\\\\data_dic.csv', index = False)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data_dictionary() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the data dictionary from the local storage\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the filtered data dictionary\n",
    "    \"\"\"\n",
    "    path = f'{rootdir}\\\\stored_data'\n",
    "    data_dic = pd.read_csv(f'{path}\\\\data_dic.csv')\n",
    "\n",
    "    return data_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_roles() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the user roles from the redcap_user_roles table in the mariaDB server.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the user roles.\n",
    "    \"\"\"\n",
    "    table_data = retrieve_database_table(['redcap_user_roles'])\n",
    "    user_roles = table_data['redcap_user_roles']\n",
    "    \n",
    "    return user_roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_user_roles() -> None:\n",
    "    \"\"\"\n",
    "    Stores the user roles locally as a CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    path = f'{rootdir}\\\\stored_data'\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    user_roles = get_user_roles()\n",
    "\n",
    "    user_roles[\"data_entry\"] = user_roles[\"data_entry\"].str.findall(r\"\\[(.*?)\\]\") \n",
    "    df_exploded = user_roles.explode(\"data_entry\", ignore_index=True)  \n",
    "    df_exploded[[\"form_name\", \"permission\"]] = df_exploded[\"data_entry\"].str.split(\",\", expand=True)\n",
    "    # df_exploded = df_exploded.drop(columns = \"data_entry\")\n",
    "\n",
    "    df_exploded.to_csv(f'{path}\\\\user_roles.csv', index = False)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_user_roles() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the user roles from the local storage\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the user roles\n",
    "    \"\"\"\n",
    "\n",
    "    path = f'{rootdir}\\\\stored_data'\n",
    "    user_roles = pd.read_csv(f'{path}\\\\user_roles.csv')\n",
    "\n",
    "    return user_roles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_information() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the user_information from the redcap_user_information table in the mariaDB server.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the user information.\n",
    "    \"\"\"\n",
    "    table_data = retrieve_database_table(['redcap_user_information'])\n",
    "    user_information = table_data['redcap_user_information']\n",
    "    \n",
    "    return user_information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_project_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves redcap_projects table from the mariaDB server and stores it locally as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the redcap_projects table.\n",
    "    \"\"\"\n",
    "    table_data = retrieve_database_table(['redcap_projects'])\n",
    "    projects = table_data['redcap_projects']\n",
    "\n",
    "    projects.to_csv('stored_data/redcap_projects.csv', index = False)\n",
    "    return projects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_project_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves redcap_projects table from the local storage.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the redcap_projects table\n",
    "    \"\"\"\n",
    "    projects = pd.read_csv('stored_data/redcap_projects.csv')\n",
    "\n",
    "    return projects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_completed_users() -> None:\n",
    "    \"\"\"\n",
    "    Stores the list of users who have completed the study in the local storage. \n",
    "    Filters to see if study_complete is 0 or ss_status is 2 or 4.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    projects = retrieve_project_data()\n",
    "    data_table_list = list(set(projects['data_table']))\n",
    "\n",
    "    table_data = retrieve_database_table(data_table_list)\n",
    "    merged_table_data = pd.concat(table_data.values())\n",
    "\n",
    "    merged_table_data = merged_table_data[((merged_table_data['field_name'] == 'study_complete') & (merged_table_data['value'] == '0')) | ((merged_table_data['field_name'] == 'ss_status') & ((merged_table_data['value'] == '2') | (merged_table_data['value'] == '4')))].reset_index(drop=True)\n",
    "\n",
    "    merged_table_data = merged_table_data[['project_id', 'record']]\n",
    "\n",
    "    with open(fr'{rootdir}\\\\stored_data\\\\completed_users.csv', 'w', newline='') as f:\n",
    "        merged_table_data.to_csv(f, index = False)\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_completed_users() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the list of users who have completed the study from the local storage.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the list of users who have completed the study\n",
    "    \"\"\"\n",
    "    completed_users = pd.read_csv(fr'{rootdir}\\\\stored_data\\\\completed_users.csv')\n",
    "    return completed_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_all_stored_data() -> None:\n",
    "    \"\"\"\n",
    "    Refreshes all stored data in the stored_data folder.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    path = f'{rootdir}\\\\stored_data'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    store_data_dictionary()\n",
    "    store_project_data()\n",
    "    store_completed_users()\n",
    "    store_user_roles()\n",
    "    logging.info(\"Stored data refreshed.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_last_checked(filename: str, message: str) -> None:\n",
    "    \"\"\"\n",
    "    Sets the last checked data in the log file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file to write the message to.\n",
    "        message (str): The message to write to the log file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(message)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_log_event_trigger(table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Refreshes (creates or replaces) a trigger for the log_event table to send data to the Flask server when a new record is created or updated.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): The name of the log_event table to create the trigger for.\n",
    "\n",
    "    Returns:\n",
    "        str: The SQL command to create the trigger for the log_event table.\n",
    "    \"\"\"\n",
    "    trigger_name = table_name + \"_update\"\n",
    "    trigger_comm = f'''CREATE OR REPLACE TRIGGER {trigger_name} AFTER INSERT ON {table_name} FOR EACH ROW BEGIN DECLARE rtn_value text DEFAULT ''; '''\n",
    "    trigger_comm += f'''SET @json = JSON_OBJECT( 'log_event_id', NEW.log_event_id, 'project_id', NEW.project_id, 'ts', NEW.ts, 'user', NEW.user, 'ip', NEW.ip, 'page', NEW.page, 'event', NEW.event, 'object_type', NEW.object_type, 'sql_log', NEW.sql_log, 'pk', NEW.pk, 'event_id', NEW.event_id, 'data_values', NEW.data_values, 'description', NEW.description, 'legacy', NEW.legacy, 'change_reason', NEW.change_reason); '''\n",
    "    trigger_comm += f'''IF (NEW.event IN ('UPDATE', 'INSERT') AND NEW.page IN ('DataEntry/index.php') AND NEW.description not in ('Assign record to Data Access Group')) THEN SELECT http_post('https://redcom.hnrc.tufts.edu/flaskApp/receive-from-maria', 'application/json', @json) INTO @rtn_value;  END IF; END; '''\n",
    "    return trigger_comm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_data_table_trigger(table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Refreshes (creates or replaces) a trigger for the data table to send data to the Flask server when a record has completed a study.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): The name of the log_event table to create the trigger for.\n",
    "\n",
    "    Returns:\n",
    "        str: The SQL command to create the trigger for the log_event table.\n",
    "    \"\"\"\n",
    "    trigger_name = table_name + \"_update\"\n",
    "    trigger_comm = f'''CREATE OR REPLACE TRIGGER {trigger_name} AFTER INSERT ON {table_name} FOR EACH ROW BEGIN DECLARE rtn_value text DEFAULT ''; '''\n",
    "    trigger_comm += f'''SET @json = JSON_OBJECT( 'project_id', NEW.project_id, 'event_id', NEW.event_id, 'record', NEW.record, 'field_name', NEW.field_name, 'value', NEW.value, 'instance', NEW.instance); '''\n",
    "    trigger_comm += f'''IF (NEW.field_name IN ('study_complete')) THEN SELECT http_post('https://redcom.hnrc.tufts.edu/flaskApp/study-complete', 'application/json', @json) INTO @rtn_value;  END IF; END; '''\n",
    "    return trigger_comm\n",
    "\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_necessary_log_event_triggers(conn: mariadb.connections.Connection) -> str:\n",
    "    \"\"\"\n",
    "    Refreshes triggers for the log_event tables to send data to the Flask server when a new record is created or updated.\n",
    "\n",
    "    Args:\n",
    "        conn (mariadb.connections.Connection): The active connection to the mariaDB server.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating the completion of the trigger creation process.\n",
    "    \"\"\"\n",
    "    cmd = 'SELECT DISTINCT log_event_table FROM redcap_projects;'\n",
    "    res = execute_maria_cmd(conn, cmd)\n",
    "    tables = []\n",
    "    for table_name in range(len(res)):\n",
    "        name = list(res[table_name])\n",
    "        tables.append(''.join(name))\n",
    "    \n",
    "    for table in tables:\n",
    "        trigger_comm = refresh_log_event_trigger(table)\n",
    "        execute_maria_cmd(conn, trigger_comm)\n",
    "\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    return f\"Last complete at {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_necessary_data_table_triggers(conn: mariadb.connections.Connection) -> str:\n",
    "    \"\"\"\n",
    "    Refreshes triggers for the log_event tables to send data to the Flask server when a new record is created or updated.\n",
    "\n",
    "    Args:\n",
    "        conn (mariadb.connections.Connection): The active connection to the mariaDB server.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating the completion of the trigger creation process.\n",
    "    \"\"\"\n",
    "    cmd = 'SELECT DISTINCT data_table FROM redcap_projects;'\n",
    "    res = execute_maria_cmd(conn, cmd)\n",
    "    tables = []\n",
    "    for table_name in range(len(res)):\n",
    "        name = list(res[table_name])\n",
    "        tables.append(''.join(name))\n",
    "    \n",
    "    for table in tables:\n",
    "        trigger_comm = refresh_data_table_trigger(table)\n",
    "        execute_maria_cmd(conn, trigger_comm)\n",
    "\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    return f\"Last complete at {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_log_event_triggers(conn: mariadb.connections.Connection) -> str:\n",
    "    \"\"\"\n",
    "    Drops triggers for the log_event tables to stop sending data to the Flask server when a new record is created or updated.\n",
    "\n",
    "    Args:\n",
    "        conn (mariadb.connections.Connection): The active connection to the mariaDB server.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating the completion of the trigger drop process.\n",
    "    \"\"\"\n",
    "    cmd = 'SELECT DISTINCT log_event_table FROM redcap_projects;'\n",
    "    res = execute_maria_cmd(conn, cmd)\n",
    "    tables = []\n",
    "    for table_name in range(len(res)):\n",
    "        name = list(res[table_name])\n",
    "        tables.append(''.join(name))\n",
    "\n",
    "    for table in tables:\n",
    "        trigger_comm = f'drop trigger if exists {table}_update;'\n",
    "        execute_maria_cmd(conn, trigger_comm)\n",
    "    \n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    return f\"Last complete at {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_data_table_triggers(conn: mariadb.connections.Connection) -> str:\n",
    "    \"\"\"\n",
    "    Drops triggers for the log_event tables to stop sending data to the Flask server when a new record is created or updated.\n",
    "\n",
    "    Args:\n",
    "        conn (mariadb.connections.Connection): The active connection to the mariaDB server.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating the completion of the trigger drop process.\n",
    "    \"\"\"\n",
    "    cmd = 'SELECT DISTINCT data_table FROM redcap_projects;'\n",
    "    res = execute_maria_cmd(conn, cmd)\n",
    "    tables = []\n",
    "    for table_name in range(len(res)):\n",
    "        name = list(res[table_name])\n",
    "        tables.append(''.join(name))\n",
    "\n",
    "    for table in tables:\n",
    "        trigger_comm = f'drop trigger if exists {table}_update;'\n",
    "        execute_maria_cmd(conn, trigger_comm)\n",
    "    \n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    return f\"Last complete at {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_background_trigger() -> None:\n",
    "    \"\"\"\n",
    "    Official process to refresh triggers for the log_event and data tables (used in multithreading).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    conn = connect_to_maria()\n",
    "    refresh_necessary_log_event_triggers(conn)\n",
    "    # refresh_necessary_data_table_triggers(conn)\n",
    "    conn.close()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_default_reviewer(project_id: int, form_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the default assignees for a project from the redcap_user_rights table.\n",
    "\n",
    "    Args:\n",
    "        project_id (int): The project_id of the project to retrieve the default assignees for.\n",
    "        form_name (str): The form_name of the form to retrieve the default assignees for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the default assignees for the project\n",
    "    \"\"\"\n",
    "    default_reviewers = pd.read_csv('stored_data/default_reviewers.csv')\n",
    "    user_roles = retrieve_user_roles()\n",
    "\n",
    "    default_reviewers = default_reviewers[default_reviewers['project_id'] == project_id]\n",
    "    # permission 0: No access, 1: View and Edit, 2: Read Only\n",
    "    user_roles = user_roles[(user_roles['project_id'] == project_id) & \n",
    "                            (user_roles['form_name'] == form_name) & \n",
    "                            (user_roles['permission'] == 1)]\n",
    "    default_reviewers = default_reviewers.merge(user_roles, on = ['project_id', 'role_name'])\n",
    "\n",
    "    return default_reviewers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_table(data_table: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the redcap_data table for operations such as joining with log table\n",
    "\n",
    "    Args:\n",
    "        data_table (pd.DataFrame): A pandas DataFrame containing the redcap_data table\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the filtered redcap_data table\n",
    "    \"\"\"        \n",
    "    data_table['instance'] = data_table['instance'].fillna(1).astype(int)\n",
    "    data_table['value'] = \"'\" + data_table['value'].astype(str) + \"'\"\n",
    "    data_table = data_table.rename(columns={'record': 'pk'})\n",
    "    data_table = data_table[['project_id', 'event_id', 'pk', 'instance', 'field_name', 'value']]\n",
    "    data_table = data_table[data_table['pk'].str.isnumeric()]\n",
    "\n",
    "    data_table = data_table.astype({'project_id': int, 'event_id': int, 'pk': int, 'instance': int, 'field_name': str, 'value': str})\n",
    "\n",
    "    return data_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_all_data(pid_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves all data from the redcap_data tables for a list of project_ids.\n",
    "\n",
    "    Args:\n",
    "        pid_list (list): A list of project_ids to retrieve the data for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing all data for the projects in the list\n",
    "    \"\"\"\n",
    "    project_table = retrieve_project_data()\n",
    "    project_table = project_table[project_table['project_id'].isin(pid_list)]\n",
    "    data_tables = project_table['data_table'].unique().tolist()\n",
    "\n",
    "    table_data = retrieve_database_table(data_tables)\n",
    "\n",
    "    unioned_data_table = pd.DataFrame(columns=['project_id', 'event_id', 'pk', 'instance', 'field_name', 'value'])\n",
    "    for data_table in range(len(data_tables)):\n",
    "        # loads relevant data table\n",
    "\n",
    "        table = filter_data_table(table_data[data_tables[data_table]])\n",
    "        unioned_data_table = unioned_data_table.append(table, ignore_index=True)\n",
    "\n",
    "    return unioned_data_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_event_and_data_tables(project_id: int) -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Retrieves the log_event and data table from the redcap_projects table.\n",
    "\n",
    "    Args:\n",
    "        project_id (int): The project_id of the project to retrieve the tables for.\n",
    "    \n",
    "    Returns:\n",
    "        tuple[list, list]: A tuple containing lists of strings representing the log_event and data table names for the project.\n",
    "    \"\"\"\n",
    "    projects = retrieve_project_data()\n",
    "    projects = projects[projects['project_id'] == project_id]\n",
    "    projects = projects[['log_event_table', 'data_table']]\n",
    "\n",
    "    log_list = list(projects['log_event_table'].unique())\n",
    "    data_list = list(projects['data_table'].unique())\n",
    "\n",
    "    return log_list, data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_confirmed_correct_fields(data_dictionary: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks the data dictionary for fields that have been confirmed correct in the redcap_data_quality_status and redcap_data_quality_resolutions tables.\n",
    "    Removes these fields from the data dictionary so they cannot be flagged as missing data. \n",
    "\n",
    "    Args:\n",
    "        data_dictionary (pd.DataFrame): A pandas DataFrame containing the data dictionary\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the data dictionary with fields that have been confirmed correct removed\n",
    "    \"\"\"\n",
    "    table_data = retrieve_database_table(['redcap_data_quality_status', 'redcap_data_quality_resolutions'])\n",
    "\n",
    "    dq_status = table_data['redcap_data_quality_status']\n",
    "    dq_resolutions = table_data['redcap_data_quality_resolutions']\n",
    "    dq_status[['record', 'event_id', 'assigned_user_id']] = dq_status[['record', 'event_id', 'assigned_user_id']].apply(pd.to_numeric, errors='coerce')\n",
    "    dq_resolutions[['res_id', 'status_id', 'user_id']] = dq_resolutions[['res_id', 'status_id', 'user_id']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    dq_total = dq_status.merge(dq_resolutions, on = 'status_id')\n",
    "    dq_total = dq_total[dq_total['response'] == 'CONFIRMED_CORRECT']\n",
    "\n",
    "    dq_total = dq_total[['project_id', 'event_id', 'field_name']].drop_duplicates()\n",
    "\n",
    "    data_dictionary = data_dictionary.merge(dq_total, on=['project_id', 'event_id', 'field_name'], how='left', indicator=True)\n",
    "    data_dictionary = data_dictionary[data_dictionary['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "\n",
    "    return data_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drw_table() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves redcap_data_quality_resolutions and redcap_data_quality_status tables and joins them\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the merged redcap_data_quality_resolutions and redcap_data_quality_status tables\n",
    "    \"\"\"\n",
    "    table_data = retrieve_database_table(['redcap_data_quality_status', 'redcap_data_quality_resolutions'])\n",
    "\n",
    "    dq_status = table_data['redcap_data_quality_status']\n",
    "    dq_resolutions = table_data['redcap_data_quality_resolutions']\n",
    "    dq_status[['record', 'event_id', 'assigned_user_id']] = dq_status[['record', 'event_id', 'assigned_user_id']].apply(pd.to_numeric, errors='coerce')\n",
    "    dq_resolutions[['res_id', 'status_id', 'user_id']] = dq_resolutions[['res_id', 'status_id', 'user_id']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    dq_total = dq_status.merge(dq_resolutions, on = 'status_id')\n",
    "    dq_total = dq_total[dq_total[['project_id', 'event_id', 'record', 'instance']].applymap(np.isfinite).all(axis=1)]\n",
    "    dq_total = dq_total.astype({'project_id': int, 'event_id': int, 'record': int, 'instance': int, 'field_name': str})\n",
    "    return dq_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_drw_count() -> int:\n",
    "    \"\"\"\n",
    "    Retrieves the current number of entries in the redcap_data_quality_status table for use in alerting\n",
    "\n",
    "    Returns: \n",
    "        int: The current number of entries in the redcap_data_quality\n",
    "    \"\"\"\n",
    "    table_data = retrieve_database_table(['redcap_data_quality_status'])\n",
    "\n",
    "    dq_status = table_data['redcap_data_quality_status']\n",
    "    status_id_count = len(dq_status.index)\n",
    "\n",
    "    return status_id_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_log_event_table(log_event_table: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the log_event table to only hold Data Entry pages rather than administrative logging.\n",
    "\n",
    "    Args:\n",
    "        log_event_table (pd.DataFrame): A pandas DataFrame containing the log_event table\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the filtered log_event table\n",
    "    \"\"\"\n",
    "    data_entry_table = log_event_table[\n",
    "        (log_event_table['description'].isin([\"Update record\", \"Create record\"])) &\n",
    "        (log_event_table['page'].isin([\"DataEntry/index.php\", \"ProjectGeneral/create_project.php\"])) &\n",
    "        (log_event_table['object_type'] == \"redcap_data\") &\n",
    "        (log_event_table['data_values'].notna() &\n",
    "        log_event_table['sql_log'].notna())\n",
    "    ]\n",
    "\n",
    "    # display(data_entry_table)\n",
    "\n",
    "    # data_values and sql_log are all grouped up in one row per form submission as a string. Splits these strings into lists for easier manipulation\n",
    "    data_entry_table['data_values'] = data_entry_table['data_values'].str.split(',\\n')\n",
    "    data_entry_table['sql_log'] = data_entry_table['sql_log'].str.split(';\\n')\n",
    "    \n",
    "    # sets default instance to 1 rather than None\n",
    "    data_entry_table['instance'] = 1\n",
    "    # data_entry_table['group_id'] = 1\n",
    "\n",
    "    # if the data_values is of an instance greater than 1, seperates the instance into a seperate column and removes it from data_values so data_values and sql_log have the same length\n",
    "    for index, row in data_entry_table.iterrows():\n",
    "        if isinstance(row['data_values'], list) and len(row['data_values']) > 0:\n",
    "            first_item = row['data_values'][0]\n",
    "            if first_item.startswith('[instance = '):\n",
    "                instance_value = int(first_item.split('=')[1].strip(']'))\n",
    "                data_entry_table.at[index, 'instance'] = instance_value\n",
    "                \n",
    "                data_value_without_instance = row['data_values'][1:]\n",
    "                data_entry_table.at[index, 'data_values'] = data_value_without_instance\n",
    "        if isinstance(row['sql_log'], list) and len(row['sql_log']) > 0:\n",
    "            first_item = row['sql_log'][0]\n",
    "            if '__GROUPID__' in first_item:\n",
    "               \n",
    "                sql_log_without_group = row['sql_log'][2:]\n",
    "                data_entry_table.at[index, 'sql_log'] = sql_log_without_group\n",
    "            \n",
    "            # second_item = row['sql_log'][1]\n",
    "            # if second_item.startswith('UPDATE redcap_edocs_metadata'):\n",
    "            sql_log_filtered = [item for item in data_entry_table.at[index, 'sql_log'] if not item.startswith('update redcap_edocs_metadata')]\n",
    "            data_entry_table.at[index, 'sql_log'] = sql_log_filtered\n",
    "    mismatched_rows = data_entry_table[data_entry_table['data_values'].apply(len) != data_entry_table['sql_log'].apply(len)]\n",
    "    mismatched_rows['len_data_values'] = mismatched_rows['data_values'].apply(len)\n",
    "    mismatched_rows['len_sql_log'] = mismatched_rows['sql_log'].apply(len)\n",
    "\n",
    "    if not mismatched_rows.empty:\n",
    "        # display(mismatched_rows)\n",
    "        logging.error(f\"Data values and SQL log lengths do not match for {len(mismatched_rows)} rows.\")\n",
    "    \n",
    "    # INSERT INTO redcap_data (project_id, event_id, record, field_name, value, instance) VALUES (131, 749, '30-1', '__GROUPID__', '30', NULL), UPDATE redcap_events_calendar SET group_id = '30' WHERE project_id = 131 AND record = '30-1', \n",
    "    data_entry_table = data_entry_table.explode(['data_values', 'sql_log']).reset_index(drop=True)\n",
    "\n",
    "    # splits field_name and value into individual columns and sets type for relevant columns\n",
    "    # print(data_entry_table)\n",
    "    if not data_entry_table.empty:\n",
    "        data_entry_table[['field_name', 'value']] = data_entry_table['data_values'].str.split(' = ',expand=True)\n",
    "        data_entry_table = data_entry_table.astype({'project_id': int, 'event_id': int, 'pk': int, 'instance': int, 'field_name': str, 'value': str})\n",
    "\n",
    "\n",
    "    return data_entry_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unioned_super_table(pid_list: list) -> pd.DataFrame:\n",
    "    \n",
    "    data_table_names = []\n",
    "    log_event_table_names = []\n",
    "    \n",
    "    for pid in pid_list:\n",
    "        log_table_name, data_table_name = get_log_event_and_data_tables(pid)\n",
    "        for data_table in data_table_name:\n",
    "            table = retrieve_database_table([data_table])\n",
    "            d_table = table[data_table]\n",
    "            d_table = d_table[d_table['project_id'] == pid]\n",
    "            d_table = filter_data_table(d_table)\n",
    "            data_table_names.append(d_table)\n",
    "        for log_table in log_table_name:\n",
    "            table = retrieve_database_table([log_table])\n",
    "            l_table = table[log_table]\n",
    "            l_table = l_table[l_table['project_id'] == pid]\n",
    "            l_table = filter_log_event_table(l_table)\n",
    "            log_event_table_names.append(l_table)\n",
    "\n",
    "\n",
    "    unioned_log_table = pd.concat(log_event_table_names)\n",
    "    unioned_data_table = pd.concat(data_table_names)\n",
    "    \n",
    "    # loads user info table and merges with log table to find user_id\n",
    "    user_info_table = retrieve_database_table(['redcap_user_information'])\n",
    "    user_info = user_info_table['redcap_user_information']\n",
    "    user_info = user_info[['ui_id', 'username', 'user_email']]\n",
    "    unioned_log_table = (unioned_log_table.merge(user_info, left_on = 'user', right_on = 'username')).drop(columns=['username'])\n",
    "\n",
    "    data_dictionary = retrieve_data_dictionary()\n",
    "    data_dictionary = data_dictionary[['project_id', 'field_name','form_name']]\n",
    "    unioned_data_table = (unioned_data_table.merge(data_dictionary, on = ['project_id', 'field_name']))\n",
    "\n",
    "    unioned_super_table = unioned_data_table.merge(unioned_log_table, on=['project_id', 'event_id', 'pk', 'instance', 'field_name', 'value'])\n",
    "\n",
    "    return unioned_super_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_open_queries(pid_list: list, production_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Resolves open queries for missing data in the redcap_data_quality_resolutions and redcap_data_quality_status tables by \n",
    "        merging with existing data and seeing what has been added.\n",
    "\n",
    "    Args:\n",
    "        pid_list (list): A list of project_ids to resolve open queries for\n",
    "        production_mode (bool, optional): If True, resolves the open queries in production mode. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logging.info(\"Resolving Open Queries.\")\n",
    "    dq_total = get_drw_table()\n",
    "\n",
    "    unioned_data_table = retrieve_all_data(pid_list)\n",
    "\n",
    "    dq_total = dq_total.merge(unioned_data_table, left_on=['project_id', 'record', 'event_id', 'field_name', 'instance'], right_on=['project_id', 'pk', 'event_id', 'field_name', 'instance'], how='left')\n",
    "\n",
    "    # find fields that have been confirmed correct or have had missing filled in, and close the query \n",
    "    confirmed_correct_list = dq_total[dq_total['response'] == 'CONFIRMED_CORRECT'][['project_id', 'event_id', 'field_name']].drop_duplicates()\n",
    "    confirmed_correct = confirmed_correct_list.merge(dq_total, on=['project_id', 'event_id', 'field_name'], how='left')\n",
    "\n",
    "    filled_in = dq_total[(dq_total['value'].notnull() & (dq_total['comment'] == \"Missing data\") & (dq_total['current_query_status'] == \"OPEN\"))].reset_index(drop=True)\n",
    "    filled_in = pd.concat([filled_in, confirmed_correct], ignore_index=True)\n",
    "    \n",
    "    conn = connect_to_maria()\n",
    "    for index, row in filled_in.iterrows():\n",
    "        if production_mode:\n",
    "            if row['query_status'] == 'OPEN':\n",
    "                sql_comm = f\"UPDATE redcap_data_quality_resolutions SET current_query_status = 'CLOSED' WHERE res_id = {row['res_id']}\"\n",
    "                execute_maria_cmd(conn, sql_comm)\n",
    "                sql_comm = f\"UPDATE redcap_data_quality_status SET query_status = 'CLOSED' WHERE status_id = {row['status_id']}\"\n",
    "                execute_maria_cmd(conn, sql_comm)\n",
    "\n",
    "                logging.info(f\"Resolved query for project_id: {row['project_id']}, record: {row['record']}, event_id: {row['event_id']}, field_name: {row['field_name']}, instance: {row['instance']}\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thread_id(mess_tables: dict, channel_name: str, user_id: int, assigned_user_id: int, project_id: int) -> int:\n",
    "    \"\"\"\n",
    "    Retrieves the thread_id of the new thread to be created in the redcap_messages_threads table.\n",
    "    \n",
    "    Args:\n",
    "        mess_tables (dict): A dictionary containing the tables necessary for the messaging system\n",
    "        channel_name (str): The name of the channel the message is being sent in\n",
    "        user_id (int): The user_id of the author of the message\n",
    "        assigned_user_id (int): The user_id of the recipient of the message\n",
    "        project_id (int): The project_id of the project the message is being sent in\n",
    "\n",
    "    Returns:\n",
    "        int: The thread_id of the new thread to be created\n",
    "    \"\"\"\n",
    "    messages_table = mess_tables['redcap_messages']\n",
    "    recipients_table = mess_tables['redcap_messages_recipients']\n",
    "    threads_table = mess_tables['redcap_messages_threads']\n",
    "    thread_id = 0\n",
    "\n",
    "    # checks if the recipient is involved in a workflow thread\n",
    "    if (np.float64(assigned_user_id) in recipients_table['recipient_user_id'].to_list()):\n",
    "        recipient_threads = list(recipients_table[recipients_table['recipient_user_id'] == assigned_user_id]['thread_id'])\n",
    "        workflow_threads = list(threads_table[(threads_table['channel_name'].isin([channel_name])) & (threads_table['project_id'] == project_id)]['thread_id'])\n",
    "        recipient_workflow_threads = list(set(recipient_threads) & set(workflow_threads))\n",
    "\n",
    "        if (len(recipient_workflow_threads) > 0):\n",
    "\n",
    "            # then checks if the author is involved in one of the same workflow threads\n",
    "            if ((np.float64(user_id) in messages_table['author_user_id'].to_list())):\n",
    "                author_threads = list(messages_table[(messages_table['author_user_id'] == user_id)]['thread_id'])\n",
    "                workflow_threads = list(threads_table[(threads_table['channel_name'].isin([channel_name])) & (threads_table['project_id'] == project_id)]['thread_id'])\n",
    "                author_workflow_threads = list(set(author_threads) & set(workflow_threads))\n",
    "\n",
    "                if (len(author_workflow_threads) > 0):\n",
    "                    thread_id = list(set(recipient_workflow_threads) & set(author_workflow_threads))[0]\n",
    "                else:\n",
    "                    thread_id = max(threads_table['thread_id']) + 1\n",
    "            \n",
    "            # if not, creates a new thread\n",
    "            else:\n",
    "                thread_id = max(threads_table['thread_id']) + 1\n",
    "        \n",
    "        # if not, creates a new thread\n",
    "        else:\n",
    "            thread_id = max(threads_table['thread_id']) + 1\n",
    "\n",
    "    # if not, creates a new thread\n",
    "    else:\n",
    "        thread_id = max(threads_table['thread_id']) + 1\n",
    "\n",
    "    return thread_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_username(mess_tables: dict, recipient_user_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the username of the recipient of the message from the redcap_user_information table.\n",
    "    \n",
    "    Args:\n",
    "        mess_tables (dict): A dictionary containing the tables necessary for the messaging system\n",
    "        recipient_user_id (int): The user_id of the recipient of the message\n",
    "\n",
    "    Returns:\n",
    "        str: The username of the recipient of the message\n",
    "    \"\"\"\n",
    "    user_info = mess_tables['redcap_user_information']\n",
    "    username = user_info[user_info['ui_id'] == recipient_user_id]['username']\n",
    "    return username.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_title(mess_tables: dict, project_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the official title of the project from the redcap_projects table.\n",
    "\n",
    "    Args:\n",
    "        mess_tables (dict): A dictionary containing the tables necessary for the messaging system\n",
    "        project_id (int): The project_id of the project to retrieve the title of\n",
    "\n",
    "    Returns:\n",
    "        str: The official title of the project\n",
    "    \"\"\"\n",
    "    project_info = mess_tables['redcap_projects']\n",
    "    app_title = project_info[project_info['project_id'] == project_id]['app_title']\n",
    "    return app_title.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_version_history() -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the build of the latest updated redcap version from the redcap_history_version table.\n",
    "\n",
    "    Returns:\n",
    "        str: The build of the latest updated redcap version\n",
    "    \"\"\"\n",
    "    version_table = retrieve_database_table(['redcap_history_version'])\n",
    "    redcap_version = version_table['redcap_version'].iloc[-1]\n",
    "    return redcap_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_msg_body(app_title: str, redcap_version:str, project_id: int, status: str, recipient_user_id: int, status_id: int, username: str, sent_time: datetime.datetime) -> str:\n",
    "    \"\"\"\n",
    "    Creates the message body to be sent to the recipient via the REDCap messenger, including a link to the workflow table.\n",
    "\n",
    "    Args:\n",
    "        app_title (str): The official title of the project\n",
    "        redcap_version (str): The current version of the redcap server\n",
    "        project_id (int): The project_id of the project the message is being sent in\n",
    "        status (str): The status of the data query\n",
    "        recipient_user_id (int): The user_id of the recipient of the message\n",
    "        status_id (int): The status_id of the data query\n",
    "        username (str): The username of the author of the message\n",
    "        sent_time (datetime.datetime): The time the message was sent\n",
    "    \n",
    "    Returns:\n",
    "        str: The message body to be sent to the recipient via the REDCap messenger, including a link to the workflow table\n",
    "    \"\"\"\n",
    "    if (status is None):\n",
    "        status = ''\n",
    "    message_body = '''[{\"msg_body\":'''\n",
    "    message_body += fr'''\"You have been assigned to a data query that was just opened in the REDCap project \\\"<b>{app_title}<\\/b>\\\".<br><br>Open the data query assigned to you: '''\n",
    "    message_body += fr'''https:\\/\\/rctest.hnrc.tufts.edu\\/redcap_v{redcap_version}\\/DataQuality\\/resolve.php?pid={project_id}&status_type=OPEN&field_rule_filter=&assigned_user_id={recipient_user_id}\"'''\n",
    "    # message_body += fr'''https:\\/\\/rctest.hnrc.tufts.edu\\/redcap_v{redcap_version}\\/DataQuality\\/resolve.php?pid={project_id}&status_type={status}&assigned_user_id={recipient_user_id}&status_id={status_id}\"'''\n",
    "    message_body += fr''',\"msg_end\":\"\",\"important\":\"0\",\"action\":\"post\",\"user\":\"{username}\",\"ts\":\"{sent_time.strftime('%m-%d-%Y %H:%M')}\"'''\n",
    "    message_body += \"\"\"}]\"\"\"\n",
    "    return message_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arm_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves and merges redcap_events_metadata and redcap_events_arms tables to match event_id and event names\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the merged redcap_events_metadata and redcap_events_arms tables\n",
    "    \"\"\"\n",
    "    table_data = retrieve_database_table(['redcap_events_metadata', 'redcap_events_arms', 'redcap_events_forms', 'redcap_events_repeat'])\n",
    "\n",
    "    metadata = table_data['redcap_events_metadata']\n",
    "    arms = table_data['redcap_events_arms']\n",
    "    forms = table_data['redcap_events_forms']\n",
    "    repeat = table_data['redcap_events_repeat']\n",
    "    \n",
    "    event_arms_table = (metadata.merge(arms, how='left', on='arm_id')) \n",
    "    event_arms_table = event_arms_table.merge(forms, how='left', on='event_id')\n",
    "    event_arms_table = event_arms_table.merge(repeat, how='left', on=['event_id', 'form_name'])\n",
    "    event_arms_table['event_name'] = (event_arms_table['descrip'].str.lower().str.replace(':','').str.replace(' ', '_').str.replace('-', '')) +\"_\"+ (event_arms_table['arm_name'].str.lower().str.replace(' ', '_').str.replace('-', ''))\n",
    "\n",
    "    return event_arms_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_periodic_email() -> None:\n",
    "    \"\"\"\n",
    "    Sends an email to all users who have unresolved data quality queries in the system. The email is sent once every 24 hours once the interval is triggered.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open('stored_data/last_email_blast.log', 'r') as file:\n",
    "        file_contents = file.read()\n",
    "    \n",
    "    timestamp = datetime.datetime.strptime(file_contents, '%Y-%m-%d %H:%M:%S')\n",
    "    time_diff = datetime.datetime.now() - timestamp\n",
    "\n",
    "    logging.info(f\"Time since last email blast: {time_diff}\")\n",
    "\n",
    "    if time_diff > datetime.timedelta(hours=24):\n",
    "        drw_table = get_drw_table()\n",
    "        user_info = get_user_information()\n",
    "        redcap_version = find_version_history()\n",
    "\n",
    "        user_info = user_info[['ui_id', 'username', 'user_email']]\n",
    "        drw_table = drw_table.merge(user_info, left_on='assigned_user_id', right_on='ui_id', how='left')\n",
    "        drw_table = drw_table[drw_table['current_query_status'] != 'CLOSED']\n",
    "\n",
    "        drw_table['ts'] = pd.to_datetime(drw_table['ts']).dt.tz_localize('UTC')\n",
    "        current_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "        drw_table['time_since'] = current_time - drw_table['ts']\n",
    "        drw_table = drw_table[drw_table['time_since'] > datetime.timedelta(hours=24)]\n",
    "        \n",
    "        drw_table = drw_table.groupby(['project_id', 'assigned_user_id', 'username', 'user_email']).size().reset_index(name='count')\n",
    "\n",
    "        for index, row in drw_table.iterrows():\n",
    "            link = f\"\"\"https://rctest.hnrc.tufts.edu/redcap_v{redcap_version}/DataQuality/resolve.php?pid={row['project_id']}&status_type=OPEN&field_rule_filter=&assigned_user_id={row['assigned_user_id']} \\n\\n\"\"\"\n",
    "            message = f\"Hello {row['username']},\\n\\nYou have {row['count']} unresolved data quality queries in project {row['project_id']}. Please log in to the system to resolve them.\\n\\n {link}Thank you,\\n\\nThe REDCap Data Quality Team\"\n",
    "            send_email(message, [row['user_email']])\n",
    "        \n",
    "        with open('stored_data/last_email_blast.log', 'w') as file:\n",
    "            file.write(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_drw_enabled(pid_list: list) -> None:\n",
    "    \"\"\"\n",
    "    Checks if the data resolution workflow parameter is enabled for the projects in the list.\n",
    "\n",
    "    Args:\n",
    "        pid_list (list): A list of project_ids to check the data resolution workflow for\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    project_table = retrieve_project_data()\n",
    "    project_table = project_table[['project_id', 'data_resolution_enabled']]\n",
    "    project_table = project_table[project_table['project_id'].isin(pid_list)]\n",
    "    \n",
    "    for index, row in project_table.iterrows():\n",
    "        if row['data_resolution_enabled'] == 2:\n",
    "            pass\n",
    "        else:\n",
    "            send_error_email(message=f\"Data resolution workflow is not enabled for project {row['project_id']}.\")\n",
    "            logging.info(f\"Data resolution workflow is not enabled for project {row['project_id']}.\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mess_data(mess_tables: dict, author_user_id: int, recipient_user_id: int, sent_time: datetime.datetime, project_id: int, status: str, status_id: int) -> list:\n",
    "    \"\"\"\n",
    "    Prepares the necessary data to be entered into the redcap_messages, redcap_messages_recipients, and redcap_messages_threads tables.\n",
    "\n",
    "    Args:\n",
    "        mess_tables (dict): A dictionary containing the tables necessary for the messaging system\n",
    "        author_user_id (int): The user_id of the author of the message\n",
    "        recipient_user_id (int): The user_id of the recipient of the message\n",
    "        sent_time (datetime.datetime): The time the message was sent\n",
    "        project_id (int): The project_id of the project the message is being sent in\n",
    "        status (str): The status of the data query\n",
    "        status_id (int): The status_id of the data query\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of tuples containing the necessary data to be entered into the redcap_messages, redcap_messages_recipients, and redcap_messages_threads tables\n",
    "\n",
    "    \"\"\"\n",
    "    # sets necessary field values for messages table\n",
    "    if (len(mess_tables['redcap_messages']) == 0):\n",
    "        message_id = 1\n",
    "    else:\n",
    "        message_id = max(mess_tables['redcap_messages']['message_id']) + 1\n",
    "    attachment_doc_id = None\n",
    "    stored_url = None\n",
    "\n",
    "    app_title = get_app_title(mess_tables, project_id)\n",
    "    channel_name = f'Assigned to a data query in project {project_id}: {app_title}'\n",
    "    redcap_version = find_version_history()\n",
    "\n",
    "    # sets necessary field values for threads table\n",
    "    thread_id = get_thread_id(mess_tables, channel_name, author_user_id, recipient_user_id, project_id)\n",
    "    type = 'CHANNEL'\n",
    "    invisible = 0\n",
    "    archived = 0\n",
    "\n",
    "    update_thread = False\n",
    "    # checks if a new thread is created. \n",
    "    if (thread_id == (max(mess_tables['redcap_messages_threads']['thread_id']) + 1)):\n",
    "        update_thread = True\n",
    "    \n",
    "    # sets necessary field values for recipients table\n",
    "    if (thread_id > max(mess_tables['redcap_messages_recipients']['thread_id'])):\n",
    "        recipient_id = max(mess_tables['redcap_messages_recipients']['recipient_id']) + 1\n",
    "    else:\n",
    "        rec_index = list(mess_tables['redcap_messages_recipients']['thread_id']).index(thread_id)\n",
    "        recipient_id = mess_tables['redcap_messages_recipients']['recipient_id'][rec_index]\n",
    "    all_users = 0\n",
    "    prioritize = 0\n",
    "    conv_leader = 1\n",
    "\n",
    "    # message sent in messenger\n",
    "    username = get_username(mess_tables, recipient_user_id)\n",
    "    message_body = create_msg_body(app_title, redcap_version, project_id, status, recipient_user_id, status_id, username, sent_time)\n",
    "    \n",
    "    # organizes full row of data for input in respective tables\n",
    "    threads_tuple = (thread_id, type, channel_name, invisible, archived, project_id)\n",
    "    messages_tuple = (message_id, thread_id, sent_time.strftime('%Y-%m-%d %H:%M:%S'), author_user_id, message_body, attachment_doc_id, stored_url)\n",
    "    recipients_tuple = (recipient_id, thread_id, recipient_user_id, all_users, prioritize, conv_leader)\n",
    "\n",
    "    data_rows = [update_thread, threads_tuple, messages_tuple, recipients_tuple]\n",
    "\n",
    "    return data_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_drw_data(dq_tables: dict, ts: datetime.datetime, project_id: int, event_id: int, hnrcid: int, field_name: str, value: str, repeat_instance: int, assigned_user_id: int, user_id: int, comment: str) -> list:\n",
    "    \"\"\"\n",
    "    Prepares the necessary data to be entered into the redcap_data_quality_status and redcap_data_quality_resolutions tables.\n",
    "\n",
    "    Args:\n",
    "        dq_tables (dict): A dictionary containing the tables necessary for the data resolution workflow\n",
    "        ts (datetime.datetime): The time the data query was entered\n",
    "        project_id (int): The project_id of the data entry\n",
    "        event_id (int): The event_id of the data entry\n",
    "        hnrcid (int): The hnrcid of the data entry\n",
    "        field_name (str): The field_name of the data entry\n",
    "        value (str): The value of the data entry\n",
    "        repeat_instance (int): The repeat_instance of the data entry\n",
    "        assigned_user_id (int): The user_id of the recipient of the data query\n",
    "        user_id (int): The user_id of the author of the data query\n",
    "        comment (str): The comment of the data query\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of tuples containing the necessary data to be entered into the redcap_data_quality_status and redcap_data_quality_resolutions tables\n",
    "    \"\"\"\n",
    "    # DQ (data quality) tables: redcap_data_quality_status, redcap_data_quality_resolutions\n",
    "    # DQ_status is where the individual flags are housed\n",
    "    # DQ_resolutions is where the Data Resolution workflow is housed, using DQ_status entries\n",
    "\n",
    "    # sets up fields for entry in DQ Status and Resolution tables\n",
    "    # many fields are pre-set, while others can be passed in, such as the specific entries to modify\n",
    "    # sets necessary field values for Status table\n",
    "    if (len(dq_tables['redcap_data_quality_status']) == 0):\n",
    "        status_id = 1\n",
    "    else:\n",
    "        status_id = max(dq_tables['redcap_data_quality_status']['status_id']) + 1\n",
    "    rule_id = None\n",
    "    pd_rule_id = None\n",
    "    non_rule = 1\n",
    "    status = None\n",
    "    exclude = 0\n",
    "    query_status = 'OPEN'\n",
    "    repeat_instrument = None\n",
    "\n",
    "    # sets necessary field values for Resolution table\n",
    "    if (len(dq_tables['redcap_data_quality_resolutions']) == 0):\n",
    "        res_id = 1\n",
    "    else:\n",
    "        res_id = max(dq_tables['redcap_data_quality_resolutions']['res_id']) + 1\n",
    "    response_requested = 1\n",
    "    response = None\n",
    "    current_query_status = 'OPEN'\n",
    "    upload_doc_id = None\n",
    "    field_comment_edited = 0\n",
    "\n",
    "    if pd.isna(repeat_instrument):\n",
    "        repeat_instrument = None\n",
    "    \n",
    "    # organizes full row of data for input in respective tables\n",
    "    status_tuple = (status_id, rule_id, pd_rule_id, non_rule, project_id, hnrcid, event_id, field_name, repeat_instrument, repeat_instance, status, exclude, query_status, assigned_user_id)\n",
    "    resolution_tuple = (res_id, status_id, ts.strftime('%Y-%m-%d %H:%M:%S'), user_id, response_requested, response, comment, current_query_status, upload_doc_id, field_comment_edited)\n",
    "\n",
    "    data_rows = [status_tuple, resolution_tuple]\n",
    "\n",
    "    return data_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry_of_outlier(unioned_super_table: pd.DataFrame, project_id: int, event_id: int, hnrcid: int, form_name: str, field_name: str, value: str, repeat_instance: int) -> tuple[int,str,str]:\n",
    "    \"\"\"\n",
    "    Retrieves the user_id, username, and email of the data entrist by filtering on the hnrcid, event_id, field_name, and instance attributes.\n",
    "    Currently returns the user with the most entries in the filtered table. \n",
    "\n",
    "    Args:\n",
    "        unioned_super_table (pd.DataFrame): A pandas DataFrame containing the unioned super table\n",
    "        project_id (int): The project_id of the data entry\n",
    "        event_id (int): The event_id of the data entry\n",
    "        hnrcid (int): The hnrcid of the data entry\n",
    "        form_name (str): The form_name of the data entry\n",
    "        field_name (str): The field_name of the data entry\n",
    "        value (str): The value of the data entry\n",
    "        repeat_instance (int): The repeat_instance of the data entry\n",
    "\n",
    "    Returns:\n",
    "        tuple[int,str,str]: A tuple containing the user_id, username, and email of the data entry\n",
    "    \"\"\"\n",
    "\n",
    "    # table with all of that patient's data entries for that project. requires a user to proceed. needs to be one entry. \n",
    "    # outlier['event_id'] needs to be equal to event_id\n",
    "    # outlier['instance'] needs to be equal to repeat_instance if instance_value exists\n",
    "    # outlier['pk'] needs to be equal to hnrcid\n",
    "    # if there are more than one user for that hnrcid for that event, make outlier['user'] whoever has greater value_counts with that hnrcid\n",
    "    # if the field_name exists for that event and pk, make outlier['field_name'] the field name. \n",
    "\n",
    "    # print(f\"{project_id} {event_id} {hnrcid} {field_name} {value} {repeat_instance}\")\n",
    "\n",
    "    if len(unioned_super_table[(unioned_super_table['form_name'] == form_name)]) > 0:\n",
    "        unioned_super_table = unioned_super_table[(unioned_super_table['form_name'] == form_name)]\n",
    "        \n",
    "    if len(unioned_super_table[(unioned_super_table['event_id'].astype(int) == int(event_id))]) > 0:\n",
    "        unioned_super_table = unioned_super_table[(unioned_super_table['event_id'].astype(int) == int(event_id))]\n",
    "\n",
    "    if len(unioned_super_table[(unioned_super_table['field_name'] == field_name)]) > 0:\n",
    "        unioned_super_table = unioned_super_table[(unioned_super_table['field_name'] == field_name)]\n",
    "\n",
    "    if len(unioned_super_table[(unioned_super_table['pk'].astype(int) == int(hnrcid))]) > 0:\n",
    "        unioned_super_table = unioned_super_table[(unioned_super_table['pk'].astype(int) == int(hnrcid))]\n",
    "\n",
    "    if len(unioned_super_table[(unioned_super_table['instance'].astype(int) == int(repeat_instance))]) > 0:\n",
    "        unioned_super_table = unioned_super_table[(unioned_super_table['instance'].astype(int) == int(repeat_instance))]\n",
    "\n",
    "    # display(outlier)\n",
    "\n",
    "    if len(unioned_super_table) == 0:\n",
    "        unioned_super_table = retrieve_default_reviewer(project_id, form_name).reset_index(drop=True)\n",
    "        first_entry = unioned_super_table.iloc[0]\n",
    "        official_user_id = int(first_entry['ui_id'])\n",
    "        official_username = str(first_entry['user'])\n",
    "        official_email = str(first_entry['user_email'])\n",
    "    else:\n",
    "        unioned_super_table = unioned_super_table.reset_index(drop=True)\n",
    "        max_user_id = unioned_super_table['ui_id'].value_counts().idxmax()\n",
    "        first_entry = unioned_super_table[unioned_super_table['ui_id'] == max_user_id].iloc[0]\n",
    "\n",
    "        official_user_id = int(first_entry['ui_id'])\n",
    "        official_username = str(first_entry['user'])\n",
    "        official_email = str(first_entry['user_email'])\n",
    "\n",
    "\n",
    "    return official_user_id, official_username, official_email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_chauvenet(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds outliers in a DataFrame using Chauvenet's criterion.\n",
    "    Pseudocode (chau_peirce_thomson.pdf):\n",
    "    1. Calculate mean and std\n",
    "    2. If n Â· erfc(|value - mean|/ std) < 1/2 then reject value\n",
    "    3. Repeat steps 1 and 2\n",
    "    4. Report final mean, std, and n\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to find outliers in\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the outliers\n",
    "    \"\"\"\n",
    "    N = len(df)\n",
    "    mean = df['value'].mean()\n",
    "    std_dev = df['value'].std()\n",
    "\n",
    "    df['z_score'] = (np.abs(df['value'] - mean)) / std_dev\n",
    "\n",
    "    # scipy.special.erfc - complementary error function.\n",
    "    df['probability'] = erfc(df['z_score'])\n",
    "\n",
    "    # probability threshold\n",
    "    chauvenet_criterion = 1 / (2 * N)\n",
    "\n",
    "    df['outlier'] = df['probability'] < chauvenet_criterion\n",
    "\n",
    "    outlier = df[df['outlier']]\n",
    "    \n",
    "    return outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pierce_critical_value(N) -> float:\n",
    "    \"\"\"\n",
    "    Approximate critical value based on dataset size N.\n",
    "    Pierce's criterion uses lookup tables to get the exact critical value.\n",
    "    Here, we use a rough approximation based on the dataset size.\n",
    "\n",
    "    Args:\n",
    "        N (int): The number of data points in the dataset\n",
    "    \n",
    "    Returns:\n",
    "        float: The critical value for Pierce's criterion\n",
    "    \"\"\"\n",
    "    # This is a rough approximation. Normally you'd look up exact values.\n",
    "    if N < 3:\n",
    "        return np.inf  # No outliers can be detected if less than 3 points\n",
    "    return 3 + 1.1 * np.log(N)  # Increase with log(N) for approximation\n",
    "\n",
    "def find_outliers_pierce(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds outliers in a DataFrame using Pierce's criterion.\n",
    "    Pseudocode (chau_peirce_thomson.pdf):\n",
    "    1. Calculate mean and std\n",
    "    2. Calculate Z-scores for each data point\n",
    "    3. Find the most extreme outlier (highest Z-score)\n",
    "    4. If the Z-score is greater than the critical value, remove the outlier\n",
    "    5. Repeat steps 1-4 until no more outliers are found\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to find outliers in\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the outliers\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()  # Work on a copy of the dataframe\n",
    "    N = len(df_clean)\n",
    "\n",
    "    while N > 2:  # Stop if less than 3 points remain\n",
    "        mean = df_clean['value'].mean()\n",
    "        std_dev = df_clean['value'].std()\n",
    "\n",
    "        # Z-scores for each data point\n",
    "        z_scores = np.abs((df_clean['value'] - mean) / std_dev)\n",
    "\n",
    "        # Find the most extreme outlier (highest Z-score)\n",
    "        max_z_score = z_scores.max()\n",
    "\n",
    "        # Get the Pierce's critical value for the current number of data points\n",
    "        critical_z = pierce_critical_value(N)\n",
    "\n",
    "        # Check if the most extreme point is an outlier\n",
    "        if max_z_score > critical_z:\n",
    "            # Remove the outlier from the dataset\n",
    "            df_clean = df_clean[z_scores != max_z_score]\n",
    "            N = len(df_clean)\n",
    "        else:\n",
    "            # No more outliers to remove, break the loop\n",
    "            break\n",
    "    \n",
    "    # Return the dataframe of outliers (original points minus the cleaned points)\n",
    "    outliers_df = df[~df.index.isin(df_clean.index)]\n",
    "    \n",
    "    return outliers_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qq_calc_cooks_dist(df_group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate Cook's distance for a group of data points.\n",
    "\n",
    "    Args:\n",
    "        df_group (pd.DataFrame): A group of data points to calculate Cook's distance for\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with Cook's distance added as a new column\n",
    "    \"\"\"\n",
    "    # Calculate Cook's distance\n",
    "    if len(df_group) > 1:\n",
    "        lm = ols('value ~ norm_quants', data=df_group).fit()\n",
    "        infl = lm.get_influence()\n",
    "        df_group['cooksd'] = infl.cooks_distance[0]\n",
    "    else:\n",
    "        df_group['cooksd'] = np.nan\n",
    "    return df_group\n",
    "\n",
    "def find_outliers_qq(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds outliers in a DataFrame using QQ plots and Cook's distance.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to find outliers in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing only the outliers.\n",
    "    \"\"\"\n",
    "    # Ensure 'value' is numeric to avoid issues with StandardScaler and other numerical operations\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    \n",
    "    # Group solo points (where the group size is 1)\n",
    "    solo_points = df.groupby('field_name').filter(lambda x: len(x) == 1)\n",
    "    df = df.groupby('field_name').filter(lambda x: len(x) > 1)\n",
    "\n",
    "    # Add z-score, probabilities, normal quantiles, and Cook's distance\n",
    "    df['zscore'] = df.groupby('field_name')['value'].transform(lambda x: StandardScaler().fit_transform(x.values.reshape(-1, 1)).flatten())\n",
    "    df['probs'] = df.groupby('field_name')['value'].transform(lambda x: np.arange(1, len(x)+1) / (len(x)+1))\n",
    "    df['norm_quants'] = df.groupby('field_name')['value'].transform(lambda x: norm.ppf(np.arange(1, len(x)+1) / (len(x)+1), np.mean(x), np.std(x)))\n",
    "    \n",
    "    # Apply QQ calculation with Cook's distance\n",
    "    df = df.groupby('field_name').apply(qq_calc_cooks_dist).reset_index(drop=True)\n",
    "    \n",
    "    # Rejoin solo points\n",
    "    df = pd.concat([df, solo_points], ignore_index=True)\n",
    "\n",
    "    # Sort by absolute z-score\n",
    "    df = df.sort_values(by=['field_name', 'zscore'], ascending=[True, False], key=lambda x: abs(x) if x.name == 'zscore' else x)\n",
    "    \n",
    "    # Prepare for loop\n",
    "    vars_unique = df['field_name'].unique()\n",
    "    df['trim_count'] = np.nan\n",
    "    df['qq_step'] = np.nan\n",
    "    df['qq_step_cd'] = np.nan\n",
    "\n",
    "    for v in vars_unique:\n",
    "        var_data = df[df['field_name'] == v].copy()\n",
    "        if len(var_data) > 3:\n",
    "            slopes = pd.DataFrame(columns=['trim_count', 'intercept', 'slope'])\n",
    "            \n",
    "            for i in range(len(var_data) - 2):\n",
    "                x = var_data.iloc[i:].copy()\n",
    "                x['norm_quants'] = norm.ppf((np.arange(1, len(x)+1) / (len(x)+1)), np.mean(x['value']), np.std(x['value']))\n",
    "                coeff = np.polyfit(np.sort(x['value']), x['norm_quants'], 1)\n",
    "                slopes.loc[i] = [i, coeff[1], coeff[0]]\n",
    "            \n",
    "            # Get magnitude of change from slope to slope\n",
    "            slopes['step'] = slopes['slope'].diff().abs()\n",
    "            slopes = slopes.dropna(subset=['step'])\n",
    "            \n",
    "            # Calculate Cook's distance for slopes\n",
    "            if not slopes.empty:\n",
    "                slopes['cooksd'] = sm.OLS(slopes['step'], sm.add_constant(slopes['trim_count'])).fit().get_influence().cooks_distance[0]\n",
    "            \n",
    "            # Save data\n",
    "            rep_len = len(var_data) - len(slopes)\n",
    "            df.loc[df['field_name'] == v, 'qq_step'] = list(slopes['step']) + [np.nan] * rep_len\n",
    "            df.loc[df['field_name'] == v, 'qq_step_cd'] = list(slopes['cooksd']) + [np.nan] * rep_len\n",
    "            df.loc[df['field_name'] == v, 'trim_count'] = list(slopes['trim_count']) + [np.nan] * rep_len\n",
    "\n",
    "    # Calculate outlier indicators based on Cook's distance (comparison is now within group)\n",
    "    df['qq_out'] = df.groupby('field_name')['qq_step'].transform(lambda x: x > np.nanmean(df['qq_step_cd']))\n",
    "    \n",
    "    # Fill missing values with False (not outliers)\n",
    "    df['qq_out'].fillna(False, inplace=True)\n",
    "\n",
    "    # Extract only the outliers\n",
    "    outliers = df[df['qq_out']].copy()\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_res_workflow_entry(conn: mariadb.connections.Connection, project_id: int, event_id: int, hnrcid: int, field_name: str, value: str, repeat_instance: int, assigned_user_id: int, user_id: int, ping: bool, comment: str, ts: datetime.datetime = datetime.datetime.now(datetime.timezone.utc)) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new data entry in the redcap_data_quality_status and redcap_data_quality_resolutions tables.\n",
    "\n",
    "    Args:\n",
    "        conn (mariadb.connections.Connection): The active connection to the mariaDB server\n",
    "        project_id (int): The project_id of the data entry\n",
    "        event_id (int): The event_id of the data entry\n",
    "        hnrcid (int): The hnrcid of the data entry\n",
    "        field_name (str): The field_name of the data entry\n",
    "        value (str): The value of the data entry\n",
    "        repeat_instance (int): The repeat_instance of the data entry\n",
    "        assigned_user_id (int): The user_id of the recipient of the data query\n",
    "        user_id (int): The user_id of the author of the data query\n",
    "        ping (bool): A boolean indicating whether to send a message to the recipient\n",
    "        comment (str): The comment of the data query\n",
    "        ts (datetime.datetime, optional): The time the data query was entered (default is the current time)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # redcap tables to be modified\n",
    "    dq_table_names = ['redcap_data_quality_status', 'redcap_data_quality_resolutions']\n",
    "    mess_table_names = ['redcap_messages_threads', 'redcap_messages', 'redcap_messages_recipients', 'redcap_user_information', 'redcap_projects']\n",
    "\n",
    "    dq_table_cols = get_colnames(conn, dq_table_names)\n",
    "    mess_table_cols = get_colnames(conn, mess_table_names)\n",
    "\n",
    "    dq_tables = get_table_data(conn, dq_table_cols)\n",
    "    mess_tables = get_table_data(conn, mess_table_cols)\n",
    "\n",
    "    drw_rows = prepare_drw_data(dq_tables, ts, project_id, event_id, hnrcid, field_name, value, repeat_instance, assigned_user_id, user_id, comment)\n",
    "    mess_rows = prepare_mess_data(mess_tables, author_user_id = user_id, recipient_user_id = assigned_user_id, sent_time = ts, project_id = project_id, status = drw_rows[0][10], status_id = drw_rows[0][0])\n",
    "    update_thread = mess_rows.pop(0)\n",
    "\n",
    "    mess_table_names.remove('redcap_user_information')\n",
    "    mess_table_names.remove('redcap_projects')\n",
    "    \n",
    "    # for both DQ tables, creates a row with all the necessary fields entered\n",
    "    for table in range(len(dq_table_names)):\n",
    "        table_name = dq_table_names[table]\n",
    "        col_names = dq_table_cols[table_name]\n",
    "        num_qs = ('?, ' * len(col_names))[:-2]\n",
    "        sql_comm = f\"INSERT INTO {table_name} ({', '.join(col_names)}) VALUES ({num_qs})\"\n",
    "        drw_row = drw_rows[table]\n",
    "        execute_maria_cmd(conn, sql_comm, drw_row)\n",
    "\n",
    "    # for three messages tables, creates a row with all the necessary fields entered. \n",
    "    for table in range(len(mess_table_names)):\n",
    "        table_name = mess_table_names[table]\n",
    "        col_names = mess_table_cols[table_name]\n",
    "        num_qs = ('?, ' * len(col_names))[:-2]\n",
    "        sql_comm = f\"INSERT INTO {table_name} ({', '.join(col_names)}) VALUES ({num_qs})\"\n",
    "        mess_row = mess_rows[table]\n",
    "\n",
    "        # if set to ping, and if one of the messages tables, sends the message\n",
    "        if ping:\n",
    "            if ((table_name == 'redcap_messages') or (update_thread & (table_name == 'redcap_messages_recipients')) or (update_thread & (table_name == 'redcap_messages_threads'))):\n",
    "                execute_maria_cmd(conn, sql_comm, mess_row)\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existing_drw_entry(project_id: int, event_id: int, hnrcid: int, field_name: str, official_user_id: int, repeat_instance: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Searches redcap_data_quality_status table to see if a DRW entry exists for that record already. \n",
    "    Prevents key-errors as duplicate entries cannot be added to DRW.  \n",
    "\n",
    "    Args:\n",
    "        project_id (int): The project_id of the data entry\n",
    "        event_id (int): The event_id of the data entry\n",
    "        hnrcid (int): The hnrcid of the data entry\n",
    "        field_name (str): The field_name of the data entry\n",
    "        official_user_id (int): The user_id of the recipient of the data query\n",
    "        repeat_instance (int): The repeat_instance of the data entry\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the existing DRW entry\n",
    "    \"\"\"\n",
    "    table_data = retrieve_database_table(['redcap_data_quality_status'])\n",
    "\n",
    "    dq_status = table_data['redcap_data_quality_status']\n",
    "    dq_status[['record', 'event_id', 'assigned_user_id']] = dq_status[['record', 'event_id', 'assigned_user_id']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    dq_status = dq_status[\n",
    "        (dq_status['record'].astype(int) == int(hnrcid)) &\n",
    "        (dq_status['project_id'].astype(int) == int(project_id)) &\n",
    "        (dq_status['event_id'].astype(int) == int(event_id)) &\n",
    "        (dq_status['assigned_user_id'].astype(int) == int(official_user_id)) &\n",
    "        (dq_status['instance'].astype(int) == int(repeat_instance)) &\n",
    "        (dq_status['field_name'] == field_name)]\n",
    "\n",
    "    return dq_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_data_submission(project_id: int, event_id: int, hnrcid: int, form_name: str, field_name: str, value: str, repeat_instance: int, official_user_id: int, username: str, email: str, ping: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Submits a new data entry to the redcap_data_quality_status and redcap_data_quality_resolutions tables after checking for duplicates.\n",
    "\n",
    "    Args:\n",
    "        project_id (int): The project_id of the data entry\n",
    "        event_id (int): The event_id of the data entry\n",
    "        hnrcid (int): The hnrcid of the data entry\n",
    "        form_name (str): The form_name of the data entry\n",
    "        field_name (str): The field_name of the data entry\n",
    "        value (str): The value of the data entry\n",
    "        repeat_instance (int): The repeat_instance of the data entry\n",
    "        official_user_id (int): The user_id of the recipient of the data query\n",
    "        username (str): The username of the recipient of the data query\n",
    "        email (str): The email of the recipient of the data query\n",
    "        ping (bool, optional): A boolean indicating whether to send a message to the recipient (default is True)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    conn = connect_to_maria()\n",
    "\n",
    "    log_msg = f'project_id {project_id}, hnrcid {hnrcid}, event_id {event_id}, repeat_instance {repeat_instance}, field_name {field_name}, value {value}, user {official_user_id}: {username} {email}. '\n",
    "\n",
    "    dq_status = check_existing_drw_entry(project_id, event_id, hnrcid, field_name, official_user_id, repeat_instance)\n",
    "\n",
    "    if (len(dq_status.index) == 0):\n",
    "        try:\n",
    "            create_data_res_workflow_entry(conn, project_id, event_id, hnrcid, field_name, value, repeat_instance, official_user_id, official_user_id, comment = f\"Flagged Value\", ping = ping)\n",
    "            log_msg += \"Created a Data Resolution Workflow entry. \"\n",
    "            if ping:\n",
    "                log_msg += f\"Sent ping to user {official_user_id}: {username}\"\n",
    "                # send_email(\"Flagged Value\", [email])\n",
    "            else:\n",
    "                log_msg += f\"Did not ping user {official_user_id}: {username}\"\n",
    "        except:\n",
    "            log_msg += \"Already exists as a Data Resolution Workflow entry. \"\n",
    "    else:\n",
    "        log_msg += \"Already exists as a Data Resolution Workflow entry. \"\n",
    "    \n",
    "    logging.info(log_msg)\n",
    "    \n",
    "    conn.commit()       # commit changes to the database so they can be officially submitted once the process is over\n",
    "    conn.close()        # connection MUST be closed at end of process so it is not infinitely hanging\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operate_outlier_qc(merged_data_table: pd.DataFrame, data_entry_table: pd.DataFrame, unioned_super_table: pd.DataFrame, outlier_method: str = 'Chauvanet', production_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Operates the outlier detection and submission process for a given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        merged_data_table (pd.DataFrame): The DataFrame containing all the data for the relevant project\n",
    "        data_entry_table (pd.DataFrame): The DataFrame containing the data that has been submitted\n",
    "        unioned_super_table (pd.DataFrame): The DataFrame containing the unioned super table\n",
    "        outlier_method (str, optional): The method to use for outlier detection (default is 'Chauvanet')\n",
    "        production_mode (bool, optional): A boolean indicating whether to run in production mode (default is False)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Fetches the data dictionary and merge with the data entry table\n",
    "    data_dictionary = get_data_dictionary()\n",
    "    data_dictionary = data_dictionary[['project_id', 'field_name','form_name']]\n",
    "    data_entry_table = (data_entry_table.merge(data_dictionary, on = ['project_id', 'field_name']))\n",
    "\n",
    "    # display(data_entry_table)\n",
    "\n",
    "    field_name = data_entry_table['field_name'].iloc[0]\n",
    "    project_id = data_entry_table['project_id'].iloc[0]\n",
    "    event_id = data_entry_table['event_id'].iloc[0]\n",
    "\n",
    "\n",
    "    drw_table = get_drw_table()\n",
    "\n",
    "    merged_data_table['instance'] = merged_data_table['instance'].fillna(1).astype(int)\n",
    "    merged_data_table['outlier'] = False\n",
    "    merged_data_table = merged_data_table.astype({'project_id': int, 'event_id': int, 'record': int, 'instance': int, 'field_name': str})\n",
    "    # marks rows that have drw entries\n",
    "    merged_data_table = merged_data_table.merge(drw_table, left_on = ['project_id', 'record', 'event_id', 'field_name', 'instance'], right_on = ['project_id', 'record', 'event_id', 'field_name', 'instance'], how='left', indicator=True)\n",
    "    \n",
    "    # Parses through DataFrame based on each field_name to check for outliers\n",
    "    df_list = []\n",
    "    cols = list(data_entry_table['field_name'])\n",
    "    for col in cols:\n",
    "        df = merged_data_table[merged_data_table['field_name'] == col]\n",
    "        df['value'] = df['value'].astype(float)\n",
    "        df = (df.merge(data_dictionary, on = ['project_id', 'field_name']))\n",
    "        if len(df) > 0:\n",
    "            df_list.append(df)\n",
    "\n",
    "    # Uses Chauvenet's criterion to find outliers in each DataFrame\n",
    "    outlier_list = []\n",
    "    for df in df_list:\n",
    "        # print(df)\n",
    "        if outlier_method == 'Chauvanet':\n",
    "            # display(df)\n",
    "            outliers = find_outliers_chauvenet(df)\n",
    "        elif outlier_method == 'Pierce':\n",
    "            outliers = find_outliers_pierce(df)\n",
    "        elif outlier_method == 'QQ':\n",
    "            outliers = find_outliers_qq(df)\n",
    "        else:\n",
    "            outliers = find_outliers_chauvenet(df)\n",
    "        # logging.info(f\"Detected outlier using {outlier_method}: {outliers}\")\n",
    "        outlier_list.append(outliers)\n",
    "    \n",
    "\n",
    "\n",
    "    # Submits a data resolution workflow entry for each outlier\n",
    "    with open('stored_data/drw_entries.csv', 'a', newline='') as file:\n",
    "        for df in outlier_list:\n",
    "            if df.empty:\n",
    "                pass\n",
    "            else:\n",
    "                # needs both because database sometimes lowers case\n",
    "                df = df[~df['comment'].isin(['Flagged Value', 'Flagged value'])]\n",
    "                # df = df[df['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "                # display(df)\n",
    "            for index, rows in df.iterrows():\n",
    "                official_user_id, username, email = get_entry_of_outlier(unioned_super_table, rows['project_id'], rows['event_id'], rows['record'], rows['form_name'], rows['field_name'], rows['value'], rows['instance'])\n",
    "                # if production_mode:\n",
    "                    # logging.info(f\"Detected outlier using {outlier_method}: {rows['project_id']}: {rows['event_id']}, {rows['record']} - {rows['field_name']} - {rows['value']} - {rows['instance']}\")\n",
    "                    # outlier_data_submission(rows['project_id'], rows['event_id'], rows['record'], rows['form_name'], rows['field_name'], rows['value'], rows['instance'], official_user_id, username, email)\n",
    "                file.write(f\"{rows['project_id']},{rows['event_id']},{rows['record']},{rows['form_name']},{rows['field_name']},{rows['value']},{rows['instance']},{official_user_id},{username},{email},\\n\")\n",
    "\n",
    "    logging.info(f\"Completed outlier detection and submission for project {project_id} and field {field_name}.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry_of_missing(unioned_super_table: pd.DataFrame, project_id: int, event_id: int, hnrcid: int, form_name: str, field_name: str, value: str, repeat_instance: int) -> tuple[int,str,str]:\n",
    "    \"\"\"\n",
    "    Retrieves the user_id and username of the data entrist by filtering on the hnrcid, event_id, field_name, and instance attributes.\n",
    "\n",
    "    Args:\n",
    "        unioned_super_table (pd.DataFrame): The DataFrame containing all the data for the relevant project\n",
    "        project_id (int): The project_id of the data entry\n",
    "        event_id (int): The event_id of the data entry\n",
    "        hnrcid (int): The hnrcid of the data entry\n",
    "        form_name (str): The form_name of the data entry\n",
    "        field_name (str): The field_name of the data entry\n",
    "        value (str): The value of the data entry\n",
    "        repeat_instance (int): The repeat_instance of the data entry\n",
    "\n",
    "    Returns:\n",
    "        tuple[int,str,str]: A tuple containing the user_id, username, and email of the data entry\n",
    "    \"\"\"\n",
    "    # imported studies have multiple project_ids and therefore need to be joined before processing\n",
    "\n",
    "\n",
    "    # display(unioned_super_table)\n",
    "\n",
    "    # table with all of that patient's data entries for that project. requires a user to proceed. needs to be one entry. \n",
    "    # missing['event_id'] needs to be equal to event_id\n",
    "    # missing['instance'] needs to be equal to repeat_instance if instance_value exists\n",
    "    # missing['pk'] needs to be equal to hnrcid\n",
    "    # if there are more than one user for that hnrcid for that event, make missing['user'] whoever has greater value_counts with that hnrcid\n",
    "    # if the field_name exists for that event and pk, make missing['field_name'] the field name. \n",
    "\n",
    "    # print(f\"{project_id} {event_id} {hnrcid} {field_name} {value} {repeat_instance}\")\n",
    "\n",
    "    # unioned_super_table.to_csv(f'output_logs/user_id/unioned_super_table_{project_id}_{event_id}_{hnrcid}_{form_name}_{field_name}_{value}_{repeat_instance}_before.csv')\n",
    "\n",
    "    if len(unioned_super_table[(unioned_super_table['form_name'] == form_name)]) > 0:\n",
    "        unioned_super_table = unioned_super_table[(unioned_super_table['form_name'] == form_name)]\n",
    "        \n",
    "    if len(unioned_super_table[(unioned_super_table['event_id'].astype(int) == int(event_id))]) > 0:\n",
    "        unioned_super_table = unioned_super_table[(unioned_super_table['event_id'].astype(int) == int(event_id))]\n",
    "\n",
    "    if len(unioned_super_table[(unioned_super_table['pk'].astype(int) == int(hnrcid))]) > 0:\n",
    "        unioned_super_table = unioned_super_table[(unioned_super_table['pk'].astype(int) == int(hnrcid))]\n",
    "\n",
    "    if len(unioned_super_table[(unioned_super_table['instance'].astype(int) == int(repeat_instance))]) > 0:\n",
    "        unioned_super_table = unioned_super_table[(unioned_super_table['instance'].astype(int) == int(repeat_instance))]\n",
    "\n",
    "    # unioned_super_table.to_csv(f'output_logs/user_id/unioned_super_table_{project_id}_{event_id}_{hnrcid}_{form_name}_{field_name}_{value}_{repeat_instance}_after.csv')\n",
    "\n",
    "    if len(unioned_super_table) == 0:\n",
    "        unioned_super_table = retrieve_default_reviewer(project_id, form_name).reset_index(drop=True)\n",
    "        first_entry = unioned_super_table.iloc[0]\n",
    "        official_user_id = int(first_entry['ui_id'])\n",
    "        official_username = str(first_entry['user'])\n",
    "        official_email = str(first_entry['user_email'])\n",
    "    else:\n",
    "        unioned_super_table = unioned_super_table.reset_index(drop=True)\n",
    "        max_user_id = unioned_super_table['ui_id'].value_counts().idxmax()\n",
    "        first_entry = unioned_super_table[unioned_super_table['ui_id'] == max_user_id].iloc[0]\n",
    "        # first_entry = unioned_super_table.iloc[0]\n",
    "\n",
    "        official_user_id = int(first_entry['ui_id'])\n",
    "        official_username = str(first_entry['user'])\n",
    "        official_email = str(first_entry['user_email'])\n",
    "\n",
    "\n",
    "    return official_user_id, official_username, official_email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_branching_logic(personalized_data_dic: pd.DataFrame, missing_check_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function navigates the branching logic of the personalized data dictionary to remove rows that do not meet the criteria of the branching logic.\n",
    "\n",
    "    Steps:\n",
    "    1. Iterate through each row in the personalized data dictionary.\n",
    "        1a. Check if the branching logic is not null.\n",
    "        1b. Iterate through the other rows in the personalized data dictionary to check if the branching logic is met.\n",
    "        1c. Drop rows if the branching logic is confirmed to be not met.\n",
    "\n",
    "    2. Iterate through each row in the personalized data dictionary.\n",
    "        2a. Drop rows if field_name is not vital or misc HIDDEN logic criteria are met.\n",
    "    \n",
    "    3. Iterate through each row in the personalized data dictionary.\n",
    "        3a. If a row is missing, check if the row references another field in the branching logic.\n",
    "        3b. Drop the row if the referenced field is not present.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        personalized_data_dic (pd.DataFrame): The data dictionary to navigate the branching logic of for that exact event/form\n",
    "        missing_check_dict (dict): A dictionary containing the event_name, event_id, and form_name of the missing data entry\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated personalized data dictionary after navigating the branching logic\n",
    "    \"\"\"\n",
    "    event_name = missing_check_dict['event_name']\n",
    "    event_id = missing_check_dict['event_id']\n",
    "    form_name = missing_check_dict['form_name']\n",
    "\n",
    "\n",
    "    for index, row in personalized_data_dic.iterrows():\n",
    "        x = row['field_name']\n",
    "        y = row['value']\n",
    "        branching_logic = row['branching_logic']\n",
    "\n",
    "        if pd.isna(branching_logic):\n",
    "            continue\n",
    "        remove_row = False\n",
    "\n",
    "        # print(list(row))\n",
    "        for _, other_row in personalized_data_dic.iterrows():\n",
    "            other_x = other_row['field_name']\n",
    "            other_y = other_row['value']\n",
    "            try:\n",
    "                other_y = float(other_y)\n",
    "            except:\n",
    "                continue\n",
    "            if (f\" AND \" in branching_logic) or (f\" and \" in branching_logic):\n",
    "                remove_row = True\n",
    "                break\n",
    "            if (f\"[{other_x}]\" in branching_logic and (np.isnan(float(other_y)))):\n",
    "                remove_row = True\n",
    "                break\n",
    "            elif (f\"[{other_x}] = \" in branching_logic and f\"[{other_x}] = {other_y}\" not in branching_logic) or (f\"[{other_x}] = \" in branching_logic and f\"[{other_x}] = '{other_y}'\" not in branching_logic):\n",
    "                remove_row = True\n",
    "                break\n",
    "            # uses regex to find the value of any digits following a comparison operator\n",
    "            elif (f\"[{other_x}]>\" in branching_logic and len(re.findall(r\"[>]=?\\s*(-?\\d+\\.?\\d*)\", branching_logic)) > 0):\n",
    "                value = int(re.findall(r\"[>]=?\\s*(-?\\d+\\.?\\d*)\", branching_logic)[0])\n",
    "                if int(other_y) <= value:\n",
    "                    remove_row = True\n",
    "                    break\n",
    "            elif (f\"[{other_x}]<\" in branching_logic and len(re.findall(r\"[<]=?\\s*(-?\\d+\\.?\\d*)\", branching_logic)) > 0):\n",
    "                value = int(re.findall(r\"[<]=?\\s*(-?\\d+\\.?\\d*)\", branching_logic)[0])\n",
    "                if int(other_y) >= value:\n",
    "                    remove_row = True\n",
    "                    break\n",
    "            elif (f\"[{other_x}] >\" in branching_logic and len(re.findall(r\"[>]=?\\s*(-?\\d+\\.?\\d*)\", branching_logic)) > 0):\n",
    "                value = int(re.findall(r\"[>]=?\\s*(-?\\d+\\.?\\d*)\", branching_logic)[0])\n",
    "                if int(other_y) <= value:\n",
    "                    remove_row = True\n",
    "                    break\n",
    "            elif (f\"[{other_x}] <\" in branching_logic and len(re.findall(r\"[<]=?\\s*(-?\\d+\\.?\\d*)\", branching_logic)) > 0):\n",
    "                value = int(re.findall(r\"[<]=?\\s*(-?\\d+\\.?\\d*)\", branching_logic)[0])\n",
    "                if int(other_y) >= value:\n",
    "                    remove_row = True\n",
    "                    break\n",
    "            elif (f\"[{other_x}] != \" in branching_logic and f\"[{other_x}] != {other_y}\" in branching_logic) or (f\"[{other_x}] != \" in branching_logic and f\"[{other_x}] != '{other_y}'\" in branching_logic):\n",
    "                remove_row = True\n",
    "                break\n",
    "            elif (f\"[event-name] = '{event_name}'\" in branching_logic) or (f\"[event-number] = '{event_id}'\" in branching_logic):\n",
    "                continue\n",
    "            elif (f\"[event-name] = '\" in branching_logic) and (f\"{event_name}'\" not in branching_logic):\n",
    "                remove_row = True\n",
    "                break\n",
    "            elif (f\"[event-name] != '{event_name}'\" in branching_logic) or (f\"[event-number] != '{event_id}'\" in branching_logic):\n",
    "                remove_row = True\n",
    "                break\n",
    "            elif (f\"[event-name] != '\" in branching_logic) and (f\"{event_name}'\" not in branching_logic):\n",
    "                continue\n",
    "            # elif (f\"[event-number] = '\" in branching_logic) and (f\"{event_id}'\" not in branching_logic):\n",
    "            #     remove_row = True\n",
    "            #     break\n",
    "            # elif (f\"[event-number] != '\" in branching_logic) and (f\"{event_id}'\" not in branching_logic):\n",
    "            #     continue\n",
    "            elif (f\"[event-number]'\" in branching_logic):\n",
    "                remove_row = True\n",
    "                break\n",
    "            elif (f\"[{other_x}(\" in branching_logic):\n",
    "                remove_row = True\n",
    "                break\n",
    "            else:\n",
    "                remove_row = False\n",
    "        if remove_row:\n",
    "            personalized_data_dic = personalized_data_dic[personalized_data_dic['field_name'] != x].reset_index(drop=True)\n",
    "        \n",
    "    for _, row in personalized_data_dic.iterrows():\n",
    "        # or (f\"other\" in row['field_name'])\n",
    "        if (row['field_name'] == f\"{form_name}_complete\") or (f\"comment\" in row['field_name']) or (f\"note\" in row['field_name']):\n",
    "             personalized_data_dic =  personalized_data_dic[personalized_data_dic['field_name'] != row['field_name']]\n",
    "\n",
    "        if (pd.notna(row['misc'])):\n",
    "            misc_list = [f\"@IF([event-name]='{event_name}', @HIDDEN, '')\", f\"@IF([{row['field_name']}]='', @HIDDEN, '')\", f\"', '', @HIDDEN)\", f\"@CALCTEXT\", f\"@CALCDATE\",f\"@READONLY\"]\n",
    "            for misc in misc_list:\n",
    "                if misc in row['misc']:\n",
    "                    personalized_data_dic = personalized_data_dic[personalized_data_dic['field_name'] != row['field_name']]\n",
    "                    break\n",
    "            if row['misc'] == '@HIDDEN':\n",
    "                 personalized_data_dic =  personalized_data_dic[personalized_data_dic['field_name'] != row['field_name']]\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "    # print(personalized_data_dic)\n",
    "    for _, row in personalized_data_dic.iterrows():\n",
    "        if (row['missing'] == True):\n",
    "            if (pd.notna(row['branching_logic'])):\n",
    "                # searches for [] brackets in the branching logic to find potentially referenced fields\n",
    "                field_references = re.findall(r\"\\[([^\\]]+)\\]\", row['branching_logic'])\n",
    "\n",
    "                for ref in field_references:\n",
    "                    if ref not in personalized_data_dic['field_name'].values and ref != 'event-name':\n",
    "                        personalized_data_dic = personalized_data_dic[personalized_data_dic['field_name'] != row['field_name']]\n",
    "                        break\n",
    "    personalized_data_dic = personalized_data_dic.reset_index(drop=True)\n",
    "\n",
    "    return personalized_data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_data(merged_data_table: pd.DataFrame, data_dictionary: pd.DataFrame, missing_check_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds missing data entries in a DataFrame using the data dictionary and the merged data table.\n",
    "\n",
    "    Args:\n",
    "        merged_data_table (pd.DataFrame): The DataFrame containing all the data for the relevant project\n",
    "        data_dictionary (pd.DataFrame): The DataFrame containing the data dictionary for the relevant project\n",
    "        missing_check_dict (dict): A dictionary containing the project_id, event_id, form_name, and event_name of the missing data entry\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the missing data entries\n",
    "    \"\"\"\n",
    "    # missing data entries are when there is no entry in the merged_data_table for a given field_name for a given record, \n",
    "    # but there is an entry in the same form_name and event_id for that record\n",
    "    all_records_in_project = merged_data_table['record'].astype(int).unique()\n",
    "    \n",
    "    project_id = missing_check_dict['project_id']\n",
    "    event_id = missing_check_dict['event_id']\n",
    "    form_name = missing_check_dict['form_name']\n",
    "    event_name = missing_check_dict['event_name']    \n",
    "    \n",
    "    filtered_data_dic = data_dictionary[\n",
    "        (data_dictionary['project_id'] == project_id) &\n",
    "        (data_dictionary['event_id'] == event_id) &\n",
    "        (data_dictionary['form_name'] == form_name) & \n",
    "        (data_dictionary['element_type'] != 'calc')\n",
    "    ]\n",
    "    field_max = len(filtered_data_dic)\n",
    "    \n",
    "    max_instance = merged_data_table['instance'].astype(int).max()\n",
    "    # print(max_instance)\n",
    "    if np.isnan(max_instance):\n",
    "        return pd.DataFrame()\n",
    "    else:    \n",
    "        merged_data_table = merged_data_table.astype({'project_id': 'int', 'event_id': 'int', 'record': 'int', 'instance': 'int', 'field_name': 'str', 'value': 'str'})\n",
    "        merged_data_table = merged_data_table[['project_id', 'event_id', 'record', 'form_name', 'field_name', 'instance', 'value']]\n",
    "\n",
    "\n",
    "        # create df with all possible data entries for each record in the form\n",
    "        filtered_data_dic = filtered_data_dic.assign(record=[all_records_in_project] * len(filtered_data_dic))\n",
    "        filtered_data_dic = filtered_data_dic.explode('record', ignore_index=True)\n",
    "        filtered_data_dic = filtered_data_dic.loc[filtered_data_dic.index.repeat(max_instance)].reset_index(drop=True)\n",
    "        filtered_data_dic['event_name'] = event_name\n",
    "        filtered_data_dic['instance'] = list(range(1, max_instance + 1)) * (len(filtered_data_dic) // max_instance)\n",
    "\n",
    "        # merge the filtered data dictionary with the data table to find missing data entries\n",
    "        filtered_data_dic = filtered_data_dic.astype({'project_id': 'int', 'event_id': 'int', 'record': 'int', 'instance': 'int', 'form_name': 'str', 'field_name': 'str'})\n",
    "        filtered_data_dic = filtered_data_dic.merge(merged_data_table, on=['project_id', 'event_id', 'record', 'form_name', 'field_name', 'instance'], how='left', indicator='indicate')\n",
    "        filtered_data_dic['missing'] = (filtered_data_dic['indicate'] == 'left_only')\n",
    "\n",
    "        # iterate through each group of data entries to find data entries that have missing fields but are not entirely missing\n",
    "        grouped = filtered_data_dic.groupby(['project_id', 'event_id', 'record', 'form_name'])\n",
    "        missing_data_entries = []\n",
    "        for group_keys, group_df in grouped:\n",
    "            project_id, event_id, record, form_name = group_keys\n",
    "            sub_grouped = group_df.groupby(['project_id', 'event_id', 'record', 'form_name', 'instance'])\n",
    "            \n",
    "            # iterate through each instance of the form to find missing fields\n",
    "            for instance_keys, instance_df in sub_grouped:\n",
    "                _, _, _, _, instance = instance_keys\n",
    "\n",
    "                missing_fields = instance_df[instance_df['missing']]['field_name'].to_list()\n",
    "                present_fields = instance_df[instance_df['missing'] == False]['field_name'].to_list()\n",
    "\n",
    "                if 0 < len(missing_fields) < field_max:\n",
    "                    instance_df = navigate_branching_logic(instance_df, missing_check_dict)\n",
    "\n",
    "                new_field_max = len(instance_df)\n",
    "                missing_count = len(instance_df[instance_df['missing']])\n",
    "\n",
    "                if missing_count not in {new_field_max, 0}:\n",
    "                    missing_data_entries.append({\n",
    "                        'project_id': project_id, \n",
    "                        'event_id': event_id, \n",
    "                        'record': record, \n",
    "                        'form_name': form_name, \n",
    "                        'field_name': f\"{form_name}_complete\",\n",
    "                        'missing_fields': missing_fields,\n",
    "                        'present_fields': present_fields,\n",
    "                        'instance': instance\n",
    "                    })\n",
    "            \n",
    "                if missing_count == new_field_max:\n",
    "                    break\n",
    "\n",
    "        missing_data = pd.DataFrame(missing_data_entries)\n",
    "        # display(missing_data)\n",
    "\n",
    "\n",
    "        return missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data_submission(project_id: int, event_id: int, hnrcid: int, form_name: str, field_name: str, value: str, repeat_instance: int, missing_fields: list, official_user_id: int, username: str, email: str, ping: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Submits a new data entry to the redcap_data_quality_status and redcap_data_quality_resolutions tables after checking for duplicates.\n",
    "\n",
    "    Args:\n",
    "        project_id (int): The project_id of the data entry\n",
    "        event_id (int): The event_id of the data entry\n",
    "        hnrcid (int): The hnrcid of the data entry\n",
    "        form_name (str): The form_name of the data entry\n",
    "        field_name (str): The field_name of the data entry\n",
    "        value (str): The value of the data entry\n",
    "        repeat_instance (int): The repeat_instance of the data entry\n",
    "        missing_fields (list): A list of the missing fields in the data entry\n",
    "        ping (bool, optional): A boolean indicating whether to send a message to the recipient (default is True)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    conn = connect_to_maria()\n",
    "\n",
    "\n",
    "    \n",
    "    log_msg = f'project_id {project_id}, hnrcid {hnrcid}, event_id {event_id}, repeat_instance {repeat_instance}, field_name {field_name}, value {value}, user {official_user_id}: {username} {email}. '\n",
    "\n",
    "    dq_status = check_existing_drw_entry(project_id, event_id, hnrcid, field_name, official_user_id, repeat_instance)\n",
    "    \n",
    "    if (len(dq_status.index) == 0):\n",
    "        try:\n",
    "            create_data_res_workflow_entry(conn, project_id, event_id, hnrcid, field_name, value, repeat_instance, official_user_id, official_user_id, comment = f\"Missing data\", ping = ping)\n",
    "            log_msg += \"Created a Data Resolution Workflow entry. \"\n",
    "            if ping:\n",
    "                log_msg += f\"Sent ping to user {official_user_id}: {username}\"\n",
    "                # send_email(\"Missing data\", [email])\n",
    "            else:\n",
    "                log_msg += f\"Did not ping user {official_user_id}: {username}\"\n",
    "        except:\n",
    "            log_msg += \"Already exists as a Data Resolution Workflow entry. \"\n",
    "    else:\n",
    "        log_msg += \"Already exists as a Data Resolution Workflow entry. \"\n",
    "    \n",
    "    logging.info(log_msg)\n",
    "    \n",
    "    conn.commit()       # commit changes to the database so they can be officially submitted once the process is over\n",
    "    conn.close()        # connection MUST be closed at end of process so it is not infinitely hanging\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_stored_drw_entries(alert_threshold: int = 100, production_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Retrieves csv file with stored DRW entries and submits any entries that do not exist in the DRW to REDCap\n",
    "\n",
    "    Args:\n",
    "        alert_threshold (int, optional): The threshold for the number of entries to submit (default is 100)\n",
    "        production_mode (bool, optional): A boolean indicating whether to run in production mode (default is False)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    drw_table = get_drw_table()\n",
    "    potential_submissions = pd.read_csv('stored_data/drw_entries.csv')\n",
    "    potential_submissions = potential_submissions.drop_duplicates()\n",
    "    potential_submissions = potential_submissions.merge(drw_table, on=['project_id', 'event_id', 'record', 'field_name', 'instance'], how='left', indicator=True)\n",
    "    potential_submissions = potential_submissions[potential_submissions['_merge'] == 'left_only']\n",
    "    \n",
    "    logging.info(f\"{len(potential_submissions)} entries to submit to REDCap.\")\n",
    "    pid_list = potential_submissions['project_id'].unique()\n",
    "    unioned_super_table = get_unioned_super_table(pid_list)\n",
    "\n",
    "    if production_mode:\n",
    "        if len(potential_submissions) > alert_threshold:\n",
    "            send_error_email(f\"More than {alert_threshold} entries to submit to REDCap. Please check the stored_data/drw_entries.csv file.\")\n",
    "            potential_submissions = potential_submissions[~potential_submissions['approved'].isna()]\n",
    "\n",
    "        for index, row in potential_submissions.iterrows():\n",
    "            if row['value'] == f\"'Missing'\":\n",
    "                official_user_id, username, email = get_entry_of_missing(unioned_super_table, row['project_id'], row['event_id'], row['record'], row['form_name'], row['field_name'], row['value'], row['instance'])\n",
    "                missing_data_submission(row['project_id'], row['event_id'], row['record'], row['form_name'], row['field_name'], row['value'], row['instance'], row['form_name'], official_user_id, username, email)\n",
    "            else:\n",
    "                official_user_id, username, email = get_entry_of_outlier(unioned_super_table, row['project_id'], row['event_id'], row['record'], row['form_name'], row['field_name'], row['value'], row['instance'])\n",
    "                outlier_data_submission(row['project_id'], row['event_id'], row['record'], row['form_name'], row['field_name'], row['value'], row['instance'], row['form_name'], official_user_id, username, email)\n",
    "    pd.DataFrame(columns=['project_id', 'event_id', 'record', 'form_name', 'field_name', 'value', 'instance', 'official_user_id', 'username', 'email','approved']).to_csv('stored_data/drw_entries.csv', index=False)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operate_missing_qc(merged_data_table: pd.DataFrame, data_entry_table: pd.DataFrame, unioned_super_table: pd.DataFrame, production_mode: bool = False) -> None: \n",
    "    \"\"\"\n",
    "    Operates the missing data detection and submission process for a given DataFrame. Finds fields that have been filled out at least once and checks for missing data entries.\n",
    "\n",
    "    Args:\n",
    "        merged_data_table (pd.DataFrame): The merged data table containing all data entries for a given project\n",
    "        data_entry_table (pd.DataFrame): The data entry table containing the inputted data \n",
    "        production_mode (bool, optional): A boolean indicating whether to run the function in production mode (default is False)\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data_dictionary = retrieve_data_dictionary()\n",
    "    data_dictionary = data_dictionary[['project_id', 'field_name','form_name', 'field_order', 'element_type', 'element_validation_type', 'branching_logic', 'misc']]\n",
    "    data_dictionary = data_dictionary.sort_values(by=['project_id', 'form_name', 'field_order'], ignore_index=True)\n",
    "    \n",
    "    drw_table = get_drw_table()\n",
    "    event_arms_table = get_arm_data()\n",
    "    event_arms_table = event_arms_table[['project_id', 'event_id', 'event_name', 'form_name', 'custom_repeat_form_label']]\n",
    "    data_dictionary = (data_dictionary.merge(event_arms_table, on = ['project_id', 'form_name']))\n",
    "\n",
    "    data_dictionary = check_for_confirmed_correct_fields(data_dictionary)\n",
    "\n",
    "    data_entry_table = (data_entry_table.merge(data_dictionary, on = ['project_id',  'event_id', 'field_name']))\n",
    "    data_entry_table['instance'] = data_entry_table['instance'].fillna(1).astype(int)\n",
    "\n",
    "    missing_check_dict = {'project_id': None, 'event_id': None, 'form_name': None, 'event_name': None}\n",
    "\n",
    "    missing_check_dict['project_id'] = int(data_entry_table['project_id'].unique()[0])\n",
    "    missing_check_dict['event_id'] = int(data_entry_table['event_id'].unique()[0])\n",
    "    missing_check_dict['form_name'] = data_entry_table['form_name'].unique()[0]\n",
    "    missing_check_dict['event_name'] = data_entry_table['event_name'].unique()[0]\n",
    "\n",
    "    # only keep fields that are in the merged_data_table (only fields that are filled out at least once are considered)\n",
    "    dd_mask = data_dictionary[['project_id', 'event_id', 'field_name']].isin(merged_data_table[['project_id', 'event_id', 'field_name']].to_dict(orient='list')).all(axis=1)\n",
    "    data_dictionary = data_dictionary[dd_mask]\n",
    "\n",
    "    merged_data_table = merged_data_table[merged_data_table['project_id'] == missing_check_dict['project_id']]\n",
    "    merged_data_table = merged_data_table[merged_data_table['event_id'].astype(int) == missing_check_dict['event_id']]\n",
    "    merged_data_table = (merged_data_table.merge(data_dictionary, on = ['project_id', 'event_id',  'field_name']))\n",
    "    merged_data_table['instance'] = merged_data_table['instance'].fillna(1).astype(int)\n",
    "\n",
    "    merged_data_table = merged_data_table[merged_data_table['form_name'] == missing_check_dict['form_name']]\n",
    "\n",
    "    # logging.info(f\"Checking for missing data in project {missing_check_dict['project_id']} for {missing_check_dict['form_name']} form and event {missing_check_dict['event_id']} {missing_check_dict['event_name']}\")\n",
    "    # display(merged_data_table)\n",
    "\n",
    "    missing_data = find_missing_data(merged_data_table, data_dictionary, missing_check_dict)\n",
    "\n",
    "    if missing_data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        missing_data = missing_data.reset_index(drop=True)\n",
    "        missing_data = missing_data.explode('missing_fields').reset_index(drop=True)\n",
    "        # only keep one entry per project id, record, field_name, event_id, form_name, and instance\n",
    "        missing_data = missing_data.drop_duplicates(subset = ['project_id', 'record', 'event_id', 'missing_fields', 'form_name', 'instance'])\n",
    "        # merge with drw table and only keep entries that do not have a drw entry\n",
    "        missing_data = missing_data.merge(drw_table, left_on = ['project_id', 'record', 'event_id', 'missing_fields', 'instance'], right_on = ['project_id', 'record', 'event_id', 'field_name', 'instance'], how='left', indicator=True)\n",
    "        missing_data = missing_data[missing_data['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "        # display(missing_data)\n",
    "    \n",
    "    with open('stored_data/drw_entries.csv', 'a', newline='') as file:\n",
    "        for _, row in missing_data.iterrows():\n",
    "            # if production_mode:\n",
    "            #     logging.info(f\"Detected missing data in proj {row['project_id']}: event {row['event_id']}, hnrcid {row['record']}, instance {row['instance']} - missing {row['missing_fields']}\")\n",
    "            #     missing_data_submission(row['project_id'], row['event_id'], row['record'], row['form_name'], row['missing_fields'], 'Missing', row['instance'], row['missing_fields'], ping = True)\n",
    "            official_user_id, username, email = get_entry_of_missing(unioned_super_table, row['project_id'], row['event_id'], row['record'], row['form_name'], row['missing_fields'], 'Missing', row['instance'])\n",
    "            file.write(f\"{row['project_id']},{row['event_id']},{row['record']},{row['form_name']},{row['missing_fields']},'Missing',{row['instance']},{official_user_id},{username},{email},\\n\")\n",
    "\n",
    "    logging.info(f\"Completed missing data detection and submission for form {missing_check_dict['form_name']} in project {missing_check_dict['project_id']} and event {missing_check_dict['event_id']}.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operate_quality_control_individual(data_entry: dict, outlier_method: str = 'Chauvanet', outlier_qc: bool = True, missing_qc: bool = True, routine: bool = False, production_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Operates the quality control process on a data entry.\n",
    "\n",
    "    Args:\n",
    "        data_entry (dict): The data entry to operate the quality control process on\n",
    "        outlier_method (str, optional): The method to use for outlier detection (default is 'Chauvanet')\n",
    "        outlier_qc (bool, optional): A boolean indicating whether to perform outlier quality control (default is True)\n",
    "        missing_qc (bool, optional): A boolean indicating whether to perform missing data quality control (default is True)\n",
    "        routine (bool, optional): A boolean indicating whether to perform the quality control routine (default is False)\n",
    "        production_mode (bool, optional): A boolean indicating whether to run the process in production mode (default is False)\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    proj_id = data_entry['project_id']\n",
    "    data_entry_table = pd.json_normalize(data_entry)\n",
    "    if not routine:\n",
    "        data_entry_table = filter_log_event_table(data_entry_table)\n",
    "    \n",
    "    if data_entry_table.empty:\n",
    "        logging.info(f\"No data entries found with proj_id {proj_id}.\")\n",
    "        return None\n",
    "\n",
    "    data_dictionary = retrieve_data_dictionary()\n",
    "    # check data_dictionary to see if data_entry_table, event_id, and field_name are in the data dictionary\n",
    "    data_dictionary = data_dictionary[['project_id', 'field_name']]\n",
    "    data_dictionary = data_dictionary[data_dictionary['project_id'] == proj_id]\n",
    "    data_entry_table = (data_entry_table.merge(data_dictionary, on = ['project_id', 'field_name']))\n",
    "\n",
    "\n",
    "    log_table_names, data_table_names = get_log_event_and_data_tables(proj_id)\n",
    "    data_tables = retrieve_database_table(data_table_names)\n",
    "\n",
    "    completed_users = retrieve_completed_users()\n",
    "    completed_user_list = completed_users[completed_users['project_id'] == proj_id]['record'].to_list()\n",
    "\n",
    "    # Creates joined table of all redcap_data tables, and filters to only include data from the associated projects\n",
    "    merged_data_table = pd.concat(data_tables.values())\n",
    "    merged_data_table = merged_data_table[merged_data_table['project_id'].isin([proj_id])]\n",
    "    merged_data_table = merged_data_table[~merged_data_table['record'].isin(completed_user_list)]\n",
    "    \n",
    "\n",
    "    unioned_super_table = get_unioned_super_table([proj_id])\n",
    "\n",
    "    if missing_qc:\n",
    "        operate_missing_qc(merged_data_table, data_entry_table, unioned_super_table, production_mode)\n",
    "    if outlier_qc:\n",
    "        # print(data_entry_table)\n",
    "        operate_outlier_qc(merged_data_table, data_entry_table, unioned_super_table, outlier_method, production_mode)\n",
    "\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operate_quality_control_routine(data_entry: dict, merged_data_table: pd.DataFrame, unioned_super_table: pd.DataFrame, outlier_method: str = 'Chauvanet', outlier_qc: bool = True, missing_qc: bool = True, routine: bool = False, production_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Operates the quality control process on a data entry.\n",
    "\n",
    "    Args:\n",
    "        data_entry (dict): The data entry to operate the quality control process on\n",
    "        merged_data_table (pd.DataFrame): The merged data table containing all data entries for a given project\n",
    "        unioned_super_table (pd.DataFrame): The unioned super table containing all data entries for a given project\n",
    "        outlier_method (str, optional): The method to use for outlier detection (default is 'Chauvanet')\n",
    "        outlier_qc (bool, optional): A boolean indicating whether to perform outlier quality control (default is True)\n",
    "        missing_qc (bool, optional): A boolean indicating whether to perform missing data quality control (default is True)\n",
    "        routine (bool, optional): A boolean indicating whether to perform the quality control routine (default is False)\n",
    "        production_mode (bool, optional): A boolean indicating whether to run the process in production mode (default is False)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    proj_id = data_entry['project_id']\n",
    "    data_entry_table = pd.json_normalize(data_entry)\n",
    "    if not routine:\n",
    "        data_entry_table = filter_log_event_table(data_entry_table)\n",
    "    \n",
    "    if data_entry_table.empty:\n",
    "        logging.info(f\"No data entries found with proj_id {proj_id}.\")\n",
    "        return None\n",
    "\n",
    "    data_dictionary = retrieve_data_dictionary()\n",
    "    # check data_dictionary to see if data_entry_table, event_id, and field_name are in the data dictionary\n",
    "    data_dictionary = data_dictionary[['project_id', 'field_name']]\n",
    "    data_dictionary = data_dictionary[data_dictionary['project_id'] == proj_id]\n",
    "    data_entry_table = (data_entry_table.merge(data_dictionary, on = ['project_id', 'field_name']))\n",
    "\n",
    "\n",
    "    completed_users = retrieve_completed_users()\n",
    "    completed_user_list = completed_users[completed_users['project_id'] == proj_id]['record'].to_list()\n",
    "\n",
    "    # Creates joined table of all redcap_data tables, and filters to only include data from the associated projects\n",
    "    \n",
    "    merged_data_table = merged_data_table[merged_data_table['project_id'].isin([proj_id])]\n",
    "    merged_data_table = merged_data_table[~merged_data_table['record'].isin(completed_user_list)]\n",
    "    \n",
    "\n",
    "    if missing_qc:\n",
    "        operate_missing_qc(merged_data_table, data_entry_table, unioned_super_table, production_mode)\n",
    "    if outlier_qc:\n",
    "        # print(data_entry_table)\n",
    "        operate_outlier_qc(merged_data_table, data_entry_table, unioned_super_table, outlier_method, production_mode)\n",
    "\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_empty_forms(data_dictionary: pd.DataFrame, project_form_event_combos: pd.DataFrame, pid_list: list) -> list:\n",
    "    \"\"\"\n",
    "    Finds all empty forms in a given project.\n",
    "\n",
    "    one df (project_form_event_combos) has all the possible field_name and event_id combos. \n",
    "    The other (unioned_data_table) has all the pk, field_name and event_id combos that have been used. \n",
    "    If a pk has a field_name and event_id combo that is not in the used df, then it is missing.\n",
    "    \n",
    "    Args:\n",
    "        data_dictionary (pd.DataFrame): The data dictionary to use for the process\n",
    "        project_form_event_combos (pd.DataFrame): The project form event combos that need to be checked\n",
    "        pid_list (list): The list of project ids\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dataframes containing the missing forms separated by project id\n",
    "    \"\"\"    \n",
    "    missing_forms_list = []\n",
    "\n",
    "    completed_users = retrieve_completed_users()\n",
    "    drw_table = get_drw_table()\n",
    "\n",
    "    for project_id in pid_list:  \n",
    "        \n",
    "        unioned_data_table = retrieve_all_data(pid_list)\n",
    "        unioned_data_table = unioned_data_table[unioned_data_table['project_id'] == project_id]\n",
    "        # display(unioned_data_table)        \n",
    "        unioned_data_table = (unioned_data_table.merge(data_dictionary, on = ['project_id', 'event_id', 'field_name']))\n",
    "        unioned_data_table = unioned_data_table[['pk', 'project_id', 'event_id', 'form_name', 'field_name', 'field_order']].drop_duplicates()\n",
    "        unioned_data_table = unioned_data_table[(unioned_data_table['field_name'].str.contains('_complete', case=False, na=False))]\n",
    "        unioned_data_table = unioned_data_table.drop(columns=['form_name']).reset_index(drop=True)\n",
    "        unioned_data_table = unioned_data_table.sort_values(by=['pk', 'event_id', 'field_order', 'field_name']).reset_index(drop=True)\n",
    "\n",
    "        for index, row in unioned_data_table.iterrows():\n",
    "            if (project_id, row['pk']) in completed_users:\n",
    "                unioned_data_table.drop(index, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        temp_project_form_event_combos = project_form_event_combos[project_form_event_combos['project_id'] == project_id]\n",
    "        \n",
    "        pk_list = unioned_data_table['pk'].unique().tolist()\n",
    "        temp_project_form_event_combos['pk'] = np.nan\n",
    "        temp_project_form_event_combos = temp_project_form_event_combos.assign(pk=[pk_list] * len(temp_project_form_event_combos))\n",
    "        temp_project_form_event_combos = temp_project_form_event_combos.explode('pk').reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        # join so all possible combinations are present, then filter out the ones that are actually present in the data\n",
    "        temp_missing_forms = (temp_project_form_event_combos.merge(unioned_data_table, on=['project_id', 'event_id', 'field_name', 'pk'], how='left', indicator=True))\n",
    "        temp_missing_forms['missing_form'] = (temp_missing_forms['_merge'] == 'left_only')\n",
    "        temp_missing_forms = temp_missing_forms[['pk', 'project_id', 'event_id', 'form_name', 'field_name', 'missing_form']].reset_index(drop=True)\n",
    "\n",
    "        # marks forms where the pk does not have matches at all for the event\n",
    "        temp_missing_forms = temp_missing_forms.merge(unioned_data_table, left_on=['pk', 'project_id', 'event_id'], right_on=['pk', 'project_id', 'event_id'], how='left', indicator=True)\n",
    "        temp_missing_forms['pk_missing_event'] = (temp_missing_forms['_merge'] == 'left_only')\n",
    "        temp_missing_forms = temp_missing_forms[['pk', 'project_id', 'event_id', 'form_name', 'field_name_x', 'missing_form', 'pk_missing_event']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        # marks forms that exist in the drw table\n",
    "        temp_missing_forms = temp_missing_forms.merge(drw_table, left_on=['pk', 'project_id', 'event_id', 'field_name_x'], right_on=['record', 'project_id', 'event_id', 'field_name'], how='left', indicator=True)\n",
    "        temp_missing_forms['drw_exists'] = (temp_missing_forms['_merge'] == 'both')\n",
    "        temp_missing_forms = temp_missing_forms[['pk', 'project_id', 'event_id', 'form_name', 'field_name_x', 'missing_form', 'pk_missing_event', 'drw_exists']].reset_index(drop=True)\n",
    "\n",
    "        missing_forms_list.append(temp_missing_forms)\n",
    "    \n",
    "    return missing_forms_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_missing_forms(pid_list: list, ping: bool = True, production_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Filters out missing forms from the data dictionary and sends a message to the user to fill out the missing forms.\n",
    "    \n",
    "    Args:\n",
    "        pid_list (list): A list of project_ids to filter missing forms for\n",
    "        ping (bool, optional): A boolean indicating whether to send a message to the recipient (default is True)\n",
    "        production_mode (bool, optional): A boolean indicating whether to run the function in production mode (default is False)\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logging.info(\"Filtering missing forms\")\n",
    "    outlier_file_name = 'stored_data/last_checked_outlier.log'\n",
    "    missing_file_name = 'stored_data/last_checked_missing.log'\n",
    "    run_empty_forms = False\n",
    "\n",
    "    with open(outlier_file_name, 'r') as f1, open(missing_file_name, 'r') as f2:\n",
    "        last_checked_outlier = f1.read()\n",
    "        last_checked_missing = f2.read()\n",
    "\n",
    "        if (last_checked_outlier == 'Finished') and (last_checked_missing == 'Finished'):\n",
    "            run_empty_forms = True\n",
    "\n",
    "\n",
    "    if run_empty_forms:\n",
    "\n",
    "        data_dictionary = retrieve_data_dictionary()\n",
    "        data_dictionary = data_dictionary[['project_id', 'field_name','form_name', 'field_order', 'element_type', 'element_validation_type', 'branching_logic', 'misc']]\n",
    "\n",
    "        project_table = retrieve_project_data()\n",
    "        project_table = project_table[['project_id', 'investigators']]\n",
    "\n",
    "        event_arms_table = get_arm_data()\n",
    "        event_arms_table = event_arms_table[['project_id', 'event_id', 'event_name', 'form_name', 'custom_repeat_form_label']]\n",
    "        data_dictionary = (data_dictionary.merge(event_arms_table, on = ['project_id', 'form_name']))\n",
    "        data_dictionary = (data_dictionary.merge(project_table, on = ['project_id']))\n",
    "\n",
    "        data_dictionary = check_for_confirmed_correct_fields(data_dictionary)\n",
    "\n",
    "        # data_dictionary = data_dictionary[data_dictionary['investigators'] == 'auto']\n",
    "        data_dictionary = data_dictionary.sort_values(by=['project_id', 'field_order', 'event_id', 'form_name'], ignore_index=True)\n",
    "\n",
    "        # uses _complete field name for each form_name and event_id combo\n",
    "        project_form_event_combos = data_dictionary[['project_id', 'event_id', 'form_name', 'field_name']].drop_duplicates()\n",
    "        project_form_event_combos = project_form_event_combos[project_form_event_combos['field_name'] == (project_form_event_combos['form_name'].astype(str) + '_complete')]\n",
    "   \n",
    "\n",
    "        missing_forms_list = find_empty_forms(data_dictionary, project_form_event_combos, pid_list)\n",
    "\n",
    "        unioned_super_table = get_unioned_super_table(pid_list)\n",
    "\n",
    "        # if the event has entries, any empty forms are considered missing, except forms that have not been filled out yet by anyone\n",
    "        # if someone has not filled out any form in that event, then it is not considered missing\n",
    "        for i, missing_forms_project in enumerate(missing_forms_list):\n",
    "            filled_event_forms = set(missing_forms_project[missing_forms_project['missing_form'] == False][['event_id', 'form_name']].drop_duplicates().itertuples(index=False, name=None))\n",
    "            \n",
    "            with open('stored_data/drw_entries.csv', 'a', newline='') as file:\n",
    "                for index, row in missing_forms_project.iterrows():\n",
    "                    if (((row['event_id'], row['form_name']) in filled_event_forms) and (row['missing_form'] == True) and (row['pk_missing_event'] == False) and (row['drw_exists'] == False)):\n",
    "                        instance = 1\n",
    "                        official_user_id, username, email = get_entry_of_missing(unioned_super_table, row['project_id'], row['event_id'], row['pk'], row['form_name'], row['field_name_x'], 'Missing', instance)\n",
    "                        # if production_mode:\n",
    "                        #     missing_data_submission(row['project_id'], row['event_id'], row['pk'], row['form_name'], row['field_name_x'], 'Missing', instance, [row['form_name']], official_user_id, username, email, ping=ping)\n",
    "                        file.write(f\"{row['project_id']},{row['event_id']},{row['pk']},{row['form_name']},{row['field_name_x']},'Missing',{instance},{official_user_id},{username},{email},\\n\")\n",
    "            gc.collect()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_all_outliers(pid_list: list, outlier_method = 'Chauvanet', alert_threshold: int = 100, ping: bool = True, production_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Checks for all missing data entries in the data dictionary and sends an email if the number of entries exceeds the alert threshold.\n",
    "\n",
    "    Args:\n",
    "        pid_list (list): A list of project_ids to check for missing data entries\n",
    "        outlier_method (str, optional): The method to use for outlier detection (default is 'Chauvanet')\n",
    "        alert_threshold (int, optional): The threshold for the number of missing data entries to send an alert (default is 100)\n",
    "        ping (bool, optional): A boolean indicating whether to send a message to the recipient (default is True)\n",
    "        production_mode (bool, optional): A boolean indicating whether to run the function in production mode (default is False)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    data_dictionary = retrieve_data_dictionary()\n",
    "    data_dictionary = data_dictionary[['project_id', 'field_name','form_name', 'field_order', 'element_type', 'element_validation_type', 'branching_logic', 'misc']]\n",
    "\n",
    "    project_table = retrieve_project_data()\n",
    "    project_table = project_table[['project_id', 'investigators']]\n",
    "\n",
    "    event_arms_table = get_arm_data()\n",
    "    event_arms_table = event_arms_table[['project_id', 'event_id', 'event_name', 'form_name', 'custom_repeat_form_label']]\n",
    "    data_dictionary = (data_dictionary.merge(event_arms_table, on = ['project_id', 'form_name']))\n",
    "    data_dictionary = (data_dictionary.merge(project_table, on = ['project_id']))\n",
    "    data_dictionary = data_dictionary[data_dictionary['project_id'].isin(pid_list)]\n",
    "\n",
    "    data_dictionary = check_for_confirmed_correct_fields(data_dictionary)\n",
    "    data_dictionary = data_dictionary.sort_values(by=['project_id', 'event_id', 'form_name'], ignore_index=True) \n",
    "\n",
    "    data_dictionary = data_dictionary[(data_dictionary['element_validation_type'] == 'int') | (data_dictionary['element_validation_type'] == 'float')]\n",
    "    project_field_combos = data_dictionary[['project_id', 'field_name']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    unioned_super_table = get_unioned_super_table(pid_list)\n",
    "\n",
    "    data_table_names = []\n",
    "    for pid in pid_list:\n",
    "        log_table_name, data_table_name = get_log_event_and_data_tables(pid)\n",
    "        for data_table in data_table_name:\n",
    "            data_table_names.append(data_table)\n",
    "\n",
    "    data_tables = retrieve_database_table(data_table_names)\n",
    "    merged_data_table = pd.concat(data_tables.values())\n",
    "\n",
    "    redcap_data = {\n",
    "        'log_event_id': None,\n",
    "        'project_id': None,\n",
    "        'ts': None,\n",
    "        'user': None,\n",
    "        'ip': None,\n",
    "        'page': None,\n",
    "        'event': None,\n",
    "        'object_type': None,\n",
    "        'sql_log': None,\n",
    "        'pk': None,\n",
    "        'event_id': None,\n",
    "        'data_values': None,\n",
    "        'description': None,\n",
    "        'legacy': None,\n",
    "        'change_reason': None\n",
    "        }\n",
    "        \n",
    "    status_id_count = get_current_drw_count()\n",
    "    alert_sent = False\n",
    "\n",
    "    # reverse order so that the most recent projects are checked first\n",
    "    # project_field_combos = project_field_combos[::-1].reset_index(drop=True)\n",
    "\n",
    "    outlier_file_name = 'stored_data/last_checked_outlier.log'\n",
    "    missing_file_name = 'stored_data/last_checked_missing.log'\n",
    "    run_outlier = False\n",
    "\n",
    "    with open(outlier_file_name, 'r') as f1, open(missing_file_name, 'r') as f2:\n",
    "        last_checked_outlier = f1.read()\n",
    "        last_checked_missing = f2.read()\n",
    "\n",
    "        if (last_checked_missing == 'Finished'):\n",
    "            run_outlier = True\n",
    "\n",
    "    if run_outlier:\n",
    "        # remove fields that have already been checked this cycle\n",
    "        try:\n",
    "            if last_checked_outlier != 'Finished':\n",
    "                last_checked_field = last_checked_outlier.split(' ')\n",
    "                last_proj_id = int(last_checked_field[0])\n",
    "                last_field_name = last_checked_field[1]\n",
    "                last_index = project_field_combos[\n",
    "                    (project_field_combos['project_id'] == last_proj_id) &\n",
    "                    (project_field_combos['field_name'] == last_field_name)].index[0]\n",
    "            \n",
    "            project_field_combos = project_field_combos.iloc[last_index:]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        for _, row in project_field_combos.iterrows():\n",
    "            redcap_data['project_id'] = row['project_id']\n",
    "            redcap_data['field_name'] = row['field_name']\n",
    "\n",
    "            operate_quality_control_routine(redcap_data, merged_data_table, unioned_super_table, outlier_method, outlier_qc = True, missing_qc = False, routine=True, production_mode=production_mode)\n",
    "\n",
    "            set_last_checked(outlier_file_name, f\"{row['project_id']} {row['field_name']}\")\n",
    "\n",
    "            status_id_count_now = get_current_drw_count()\n",
    "            if (((status_id_count_now - status_id_count) > alert_threshold) and (not alert_sent)):\n",
    "                send_error_email(message=f\"Alarming number of DRW entries ({status_id_count_now - status_id_count}) have been created recently. Please check the DRW table.\")\n",
    "                alert_sent = True\n",
    "            \n",
    "            gc.collect()\n",
    "        set_last_checked(outlier_file_name, f\"Finished\")\n",
    "    logging.info(\"All outlier data entries have been checked.\")\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_all_missing(pid_list: list, alert_threshold: int = 100, ping: bool = True, production_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Checks for all missing data entries in the data dictionary and sends an email if the number of entries exceeds the alert threshold.\n",
    "\n",
    "    Args:\n",
    "        pid_list (list): A list of project_ids to check for missing data entries\n",
    "        alert_threshold (int, optional): The threshold for the number of missing data entries to send an alert (default is 100)\n",
    "        ping (bool, optional): A boolean indicating whether to send a message to the recipient (default is True)\n",
    "        production_mode (bool, optional): A boolean indicating whether to run the function in production mode (default is False)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data_dictionary = retrieve_data_dictionary()\n",
    "    data_dictionary = data_dictionary[['project_id', 'field_name','form_name', 'field_order', 'element_type', 'element_validation_type', 'branching_logic', 'misc']]\n",
    "\n",
    "    project_table = retrieve_project_data()\n",
    "    project_table = project_table[['project_id', 'investigators']]\n",
    "\n",
    "    event_arms_table = get_arm_data()\n",
    "    event_arms_table = event_arms_table[['project_id', 'event_id', 'event_name', 'form_name', 'custom_repeat_form_label']]\n",
    "    data_dictionary = (data_dictionary.merge(event_arms_table, on = ['project_id', 'form_name']))\n",
    "    data_dictionary = (data_dictionary.merge(project_table, on = ['project_id']))\n",
    "    data_dictionary = data_dictionary[data_dictionary['project_id'].isin(pid_list)]\n",
    "\n",
    "    data_dictionary = check_for_confirmed_correct_fields(data_dictionary)\n",
    "    data_dictionary = data_dictionary.sort_values(by=['project_id', 'event_id', 'form_name'], ignore_index=True) \n",
    "    project_form_event_combos = data_dictionary[['project_id', 'event_id', 'form_name', 'field_name']].drop_duplicates()\n",
    "\n",
    "    # uses _complete field name for each form_name and event_id combo\n",
    "    project_form_event_combos = project_form_event_combos[(project_form_event_combos['field_name'].str.contains('_complete', case=False, na=False))]\n",
    "    project_form_event_combos = project_form_event_combos.drop(columns=['form_name'])\n",
    "\n",
    "    unioned_super_table = get_unioned_super_table(pid_list)\n",
    "\n",
    "    data_table_names = []\n",
    "    for pid in pid_list:\n",
    "        log_table_name, data_table_name = get_log_event_and_data_tables(pid)\n",
    "        for data_table in data_table_name:\n",
    "            data_table_names.append(data_table)\n",
    "\n",
    "    data_tables = retrieve_database_table(data_table_names)\n",
    "    merged_data_table = pd.concat(data_tables.values())\n",
    "\n",
    "    redcap_data = {\n",
    "        'log_event_id': None,\n",
    "        'project_id': None,\n",
    "        'ts': None,\n",
    "        'user': None,\n",
    "        'ip': None,\n",
    "        'page': None,\n",
    "        'event': None,\n",
    "        'object_type': None,\n",
    "        'sql_log': None,\n",
    "        'pk': None,\n",
    "        'event_id': None,\n",
    "        'data_values': None,\n",
    "        'description': None,\n",
    "        'legacy': None,\n",
    "        'change_reason': None\n",
    "        }\n",
    "    \n",
    "    \n",
    "    status_id_count = get_current_drw_count()\n",
    "    alert_sent = False\n",
    "\n",
    "    # reverse order so that the most recent projects are checked first\n",
    "    project_form_event_combos = project_form_event_combos[::-1].reset_index(drop=True)\n",
    "    file_name = 'stored_data/last_checked_missing.log'\n",
    "\n",
    "    with open(file_name, 'r') as f:\n",
    "        last_checked_form = f.read()\n",
    "\n",
    "\n",
    "    # remove forms that have already been checked this cycle\n",
    "    try:\n",
    "        if last_checked_form != 'Finished':\n",
    "            last_checked_form = last_checked_form.split(' ')\n",
    "            last_proj_id = int(last_checked_form[0])\n",
    "            last_event_id = int(last_checked_form[1])\n",
    "            last_field_name = last_checked_form[2]\n",
    "            last_index = project_form_event_combos[\n",
    "                (project_form_event_combos['project_id'] == last_proj_id) &\n",
    "                (project_form_event_combos['event_id'] == last_event_id) &\n",
    "                (project_form_event_combos['field_name'] == last_field_name)].index[0]\n",
    "            \n",
    "            project_form_event_combos = project_form_event_combos.iloc[last_index:]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for _, row in project_form_event_combos.iterrows():\n",
    "        redcap_data['project_id'] = row['project_id']\n",
    "        redcap_data['event_id'] = row['event_id']\n",
    "        redcap_data['field_name'] = row['field_name']\n",
    "        redcap_data['instance'] = 1\n",
    "        operate_quality_control_routine(redcap_data, merged_data_table, unioned_super_table, outlier_method = '', outlier_qc = False, missing_qc = True, routine=True, production_mode=production_mode)\n",
    "\n",
    "        set_last_checked(file_name, f\"{row['project_id']} {row['event_id']} {row['field_name']}\")\n",
    "\n",
    "        status_id_count_now = get_current_drw_count()\n",
    "        if (((status_id_count_now - status_id_count) > alert_threshold) and (not alert_sent)):\n",
    "            send_error_email(message=f\"Alarming number of DRW entries ({status_id_count_now - status_id_count}) have been created recently. Please check the DRW table.\")\n",
    "            alert_sent = True\n",
    "\n",
    "        gc.collect()\n",
    "    set_last_checked(file_name, f\"Finished\")\n",
    "    logging.info(\"All missing data entries have been checked.\")\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_all_outlier_and_missing(pid_list: list, outlier_method: str = 'Chauvanet', alert_threshold: int = 100, ping: bool = True, production_mode: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Checks for all missing data entries and outlier data in the data dictionary and sends an email if the number of entries exceeds the alert threshold.\n",
    "\n",
    "    Args:\n",
    "        pid_list (list): A list of project_ids to check for missing data entries\n",
    "        outlier_method (str, optional): The method to use for outlier detection (default is 'Chauvanet')\n",
    "        alert_threshold (int, optional): The threshold for the number of missing data entries to send an alert (default is 100)\n",
    "        ping (bool, optional): A boolean indicating whether to send a message to the recipient (default is True)\n",
    "        production_mode (bool, optional): A boolean indicating whether to run the function in production mode (default is False)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    refresh_all_stored_data()\n",
    "    check_drw_enabled(pid_list)\n",
    "    if production_mode:\n",
    "        resolve_open_queries(pid_list)\n",
    "    filter_missing_forms(pid_list, ping, production_mode)\n",
    "    with open('stored_data/last_routine.log', 'w') as file:\n",
    "        file.write(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    check_for_all_outliers(pid_list, outlier_method, alert_threshold, ping, production_mode)\n",
    "    check_for_all_missing(pid_list, alert_threshold, ping, production_mode)\n",
    "    if production_mode:\n",
    "        submit_stored_drw_entries(alert_threshold, production_mode)\n",
    "    return None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
